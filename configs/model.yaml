# M3: Model Configuration

model:
  # Radio Encoder (Temporal Set Transformer)
  radio_encoder:
    num_cells: 2  # Max number of cells in dataset
    num_beams: 64   # Max number of beam IDs
    d_model: 128    # Hidden dimension (Reduced for memory)
    nhead: 4        # Attention heads
    num_layers: 3   # Transformer layers (reduced)
    dropout: 0.1
    max_seq_len: 20 # Max measurement reports per sequence
    
    # Feature dimensions
    rt_features_dim: 16   # padded to accommodate all possible RT features
    phy_features_dim: 8   # rsrp, rsrq, sinr, cqi, ri, pmi, capacity, condition_number
    mac_features_dim: 6   # serving_cell_id, timing_advance, phr, throughput, bler, + padding
    
  # Map Encoder (Vision Transformer)
  map_encoder:
    img_size: 256             # Map resolution (pixels)
    patch_size: 16            # Patch size (CRITICAL for memory efficiency)
    in_channels: 10           # 5 (radio) + 5 (OSM)
    d_model: 192              # Hidden dimension
    nhead: 4                  # Attention heads
    num_layers: 3             # Transformer layers
    dropout: 0.1
    cache_size: 32            # Cache map encodings per scene (eval-only by default)
    cache_mode: eval          # "off", "eval", "train", "always"
    
    # Channel configuration
    radio_map_channels: 5  # path_gain, toa, snr, sinr, throughput
    osm_map_channels: 5    # height, material, footprint, road, terrain
  
  # Cross-Attention Fusion
  fusion:
    d_radio: 128     # From radio encoder
    d_map: 192       # From map encoder
    d_fusion: 256    # Output dimension (reduced)
    nhead: 4
    dropout: 0.1
  
  # Coarse Head (Grid Classification)
  coarse_head:
    grid_size: 32    # 32x32 grid cells
    d_input: 256     # From fusion (Updated)
    dropout: 0.1
  
  # Fine Head (Offset Regression)
  fine_head:
    d_input: 256     # From fusion (Updated)
    d_hidden: 256
    top_k: 5         # Number of candidate cells
    patch_size: 64   # High-res patch size (pixels)
    use_local_map: true   # Condition fine head on local map patches
    offset_scale: 1.5     # Max offset in multiples of cell size
    dropout: 0.1

# Training configuration - ENHANCED for positioning
training:
  batch_size: 32
  num_epochs: 100
  learning_rate: 0.0003
  weight_decay: 0.01
  warmup_steps: 500
  
  # Loss weights - Emphasize position accuracy
  loss:
    coarse_weight: 1.5       # Increase coarse weight for grid prediction
    fine_weight: 1.0
    use_physics_loss: false
    auxiliary:
      enabled: true
      weight: 0.1
      input: radio
      tasks:
        nlos: 1.0
        num_paths: 0.5
        timing_advance: 0.5
        ta_residual: 0.3
  
  # Optimizer
  optimizer: adamw
  scheduler: cosine_with_warmup
  
  # Regularization
  dropout: 0.1
  gradient_clip: 1.0
  
  # Data augmentation (training only) - More aggressive for generalization
  augmentation:
    # Feature-level augmentations
    feature_noise: 0.05        # Lower noise (features are already noisy)
    feature_dropout: 0.1       # Less dropout
    temporal_dropout: 0.1      # Less temporal dropout
    
    # Map-level augmentations
    random_flip: true          # Random horizontal flip
    random_rotation: true      # Random 90° rotations
    scale_range: [0.95, 1.05]  # Smaller scale jitter

# Dataset configuration
dataset:
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  
  # Map parameters
  map_resolution: 2.0  # meters per pixel
  scene_extent: [0.0, 0.0, 1200.0, 1200.0]  # [x_min, y_min, x_max, y_max] in meters - FIXED to match actual scene sizes
  
  # Preprocessing
  normalize_features: true
  handle_missing_values: "mask"  # "mask", "zero", or "mean"
  sequence_length: 10
  max_cells: 2

# M4: Physics Loss Configuration (Differentiable Physics Regularization)
physics_loss:
  # Enable/disable physics loss
  enabled: false  # Set to true to use physics loss (requires precomputed radio maps)
  
  # Loss weight (λ_phys in paper)
  lambda_phys: 0.1
  
  # Feature weights (importance of each feature in physics loss)
  feature_weights:
    rsrp: 1.0         # High weight, most reliable
    rsrq: 0.5         # Medium, affected by NLOS bias
    sinr: 0.8         # High, good signal quality indicator
    cqi: 0.3          # Lower, quantized and noisy
    throughput: 0.2   # Lower, depends on scheduler/load
  
  # Loss type
  loss_type: "mse"  # "mse" or "huber"
  huber_delta: 1.0  # Only used if loss_type="huber"
  
  # Feature normalization
  normalize_features: true  # Normalize features before computing loss
  
  # Radio map paths (precomputed using scripts/generate_radio_maps.py)
  radio_maps_dir: "data/radio_maps"
  
  # Inference-time refinement (optional)
  refinement:
    enabled: true  # Enable gradient-based position refinement at inference
    num_steps: 10   # Increased steps for better convergence
    learning_rate: 0.1  # Moderate learning rate
    min_confidence_threshold: 0.6  # Refine if confidence is below 60%
    clip_to_extent: true  # Clip refined positions to map extent
    coarse_logit_temperature: 1.0  # Temperature scaling for coarse logits
    candidate_sigma_ratio: 0.05  # Hypothesis kernel width as fraction of map extent
    confidence_combine: min  # min | product | coarse | fine

# Validation/Testing
evaluation:
  metrics:
    - median_error
    - rmse
    - percentile_67
    - percentile_90
    - percentile_95
    - success_rate_5m
    - success_rate_10m
  
  # Error analysis
  error_bins: [0, 5, 10, 20, 50, 100, 200, 500]  # meters
  
  # Visualization frequency
  viz_frequency: 5  # epochs

# Infrastructure
infrastructure:
  accelerator: "gpu"
  devices: 1
  precision: "16-mixed"  # Mixed precision training
  num_workers: 2         # DataLoader workers (reduced for memory)
  
  # Checkpointing
  checkpoint:
    save_top_k: 3
    monitor: "val_median_error"
    mode: "min"
  
  # Early stopping
  early_stopping:
    patience: 10
    monitor: "val_median_error"
    mode: "min"
  
  # Logging
  logging:
    use_wandb: false
    use_comet: true   # Enable Comet ML logging (requires COMET_API_KEY env var)
    project: "transformer-ue-localization"
    log_every_n_steps: 50
