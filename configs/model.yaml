# M3: Model Configuration

model:
  # Radio Encoder (Temporal Set Transformer)
  radio_encoder:
    num_cells: 512  # Max number of unique cell IDs
    num_beams: 64   # Max number of beam IDs
    d_model: 512    # Hidden dimension
    nhead: 8        # Attention heads
    num_layers: 6   # Transformer layers
    dropout: 0.1
    max_seq_len: 20 # Max measurement reports per sequence
    
    # Feature dimensions
    rt_features_dim: 10   # path_gain, toa, aoa_az, aoa_el, aod_az, aod_el, doppler, rms_ds, k_factor, num_paths
    phy_features_dim: 8   # rsrp, rsrq, sinr, cqi, ri, pmi, l1_rsrp (per beam, variable)
    mac_features_dim: 6   # timing_advance, phr, serving_cell_id, throughput, bler, num_neighbors
  
  # Map Encoder (Vision Transformer)
  map_encoder:
    img_size: 256          # Map resolution (pixels)
    patch_size: 16         # Patch size for ViT
    in_channels: 10        # 5 (radio) + 5 (OSM)
    d_model: 768           # Hidden dimension
    nhead: 8               # Attention heads
    num_layers: 12         # Transformer layers
    dropout: 0.1
    
    # Channel configuration
    radio_map_channels: 5  # path_gain, toa, snr, sinr, throughput
    osm_map_channels: 5    # height, material_id, footprint_mask, road_mask, terrain
  
  # Cross-Attention Fusion
  fusion:
    d_radio: 512     # From radio encoder
    d_map: 768       # From map encoder
    d_fusion: 768    # Output dimension
    nhead: 8
    dropout: 0.1
  
  # Coarse Head (Grid Classification)
  coarse_head:
    grid_size: 32    # 32x32 grid cells
    d_input: 768     # From fusion
    dropout: 0.1
  
  # Fine Head (Offset Regression)
  fine_head:
    d_input: 768     # From fusion
    d_hidden: 256
    top_k: 5         # Number of candidate cells
    patch_size: 64   # High-res patch size (pixels)
    dropout: 0.1

# Training configuration
training:
  batch_size: 32
  num_epochs: 100
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_steps: 1000
  
  # Loss weights
  loss:
    coarse_weight: 1.0
    fine_weight: 1.0
    use_physics_loss: false  # Enable for M4 training (requires precomputed radio maps)
  
  # Optimizer
  optimizer: adamw
  scheduler: cosine_with_warmup
  
  # Regularization
  dropout: 0.1
  gradient_clip: 1.0
  
  # Data augmentation
  augmentation:
    rotation_degrees: 5
    scale_range: [0.9, 1.1]
    feature_dropout: 0.15
    temporal_dropout: 0.25

# Dataset configuration
dataset:
  zarr_path: "data/synthetic/dataset_*.zarr"
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  
  # Map parameters
  map_resolution: 2.0  # meters per pixel
  scene_extent: [0.0, 0.0, 512.0, 512.0]  # [x_min, y_min, x_max, y_max] in meters
  
  # Preprocessing
  normalize_features: true
  handle_missing_values: "mask"  # "mask", "zero", or "mean"

# M4: Physics Loss Configuration (Differentiable Physics Regularization)
physics_loss:
  # Enable/disable physics loss
  enabled: false  # Set to true to use physics loss (requires precomputed radio maps)
  
  # Loss weight (Î»_phys in paper)
  lambda_phys: 0.1
  
  # Feature weights (importance of each feature in physics loss)
  feature_weights:
    path_gain: 1.0    # High weight, most reliable
    toa: 0.5          # Medium, affected by NLOS bias
    aoa: 0.3          # Lower, high measurement noise
    snr: 0.8          # High, good signal quality indicator
    sinr: 0.8         # High, good signal quality indicator
    throughput: 0.2   # Lower, depends on scheduler/load
    bler: 0.2         # Lower, depends on channel conditions
  
  # Loss type
  loss_type: "mse"  # "mse" or "huber"
  huber_delta: 1.0  # Only used if loss_type="huber"
  
  # Feature normalization
  normalize_features: true  # Normalize features before computing loss
  
  # Radio map paths (precomputed using scripts/generate_radio_maps.py)
  radio_maps_dir: "data/radio_maps"
  
  # Inference-time refinement (optional)
  refinement:
    enabled: false  # Enable gradient-based position refinement at inference
    num_steps: 5    # Number of gradient descent steps
    learning_rate: 0.5  # Learning rate in meters per step
    min_confidence_threshold: 0.5  # Only refine predictions below this confidence
    clip_to_extent: true  # Clip refined positions to map extent

# Validation/Testing
evaluation:
  metrics:
    - median_error
    - rmse
    - percentile_67
    - percentile_90
    - percentile_95
    - success_rate_5m
    - success_rate_10m
  
  # Error analysis
  error_bins: [0, 5, 10, 20, 50, 100, 200, 500]  # meters
  
  # Visualization frequency
  viz_frequency: 5  # epochs

# Infrastructure
infrastructure:
  accelerator: "gpu"
  devices: 1
  precision: "16-mixed"  # Mixed precision training
  num_workers: 4         # DataLoader workers
  
  # Checkpointing
  checkpoint:
    save_top_k: 3
    monitor: "val_median_error"
    mode: "min"
  
  # Early stopping
  early_stopping:
    patience: 10
    monitor: "val_median_error"
    mode: "min"
  
  # Logging
  logging:
    use_wandb: true
    project: "transformer-ue-localization"
    log_every_n_steps: 50
