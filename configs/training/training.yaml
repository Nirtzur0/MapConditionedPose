# Milestone 3: Map-Conditioned Transformer Training Configuration

model:
  name: "MapConditionedTransformer"
  
  # Radio Encoder (Temporal Transformer)
  radio_encoder:
    type: "SetTransformer"
    num_cells: 6  # Number of cells in the dataset
    num_beams: 64
    d_model: 512
    nhead: 8  # Changed from num_heads
    num_layers: 8
    dropout: 0.1
    max_seq_len: 20
    rt_features_dim: 10
    phy_features_dim: 8
    mac_features_dim: 6
    
    # Token Embedding
    embedding:
      cell_id_embed_dim: 64
      beam_id_embed_dim: 32
      time_embed_dim: 64
      feature_dim: 256  # RT+PHY+MAC feature vector
      
    # Masking
    use_token_masking: true  # For missing measurements
    use_feature_masking: true  # For missing features within tokens
    
  # Map Encoder (Vision Transformer)
  map_encoder:
    use_e2_equivariant: true  # Use E2 equivariant vision transformer (NEW)
    num_group_elements: 8  # p4m group: 4 rotations x 2 reflections (NEW)
    img_size: 256
    patch_size: 16
    in_channels: 10
    d_model: 768
    nhead: 8
    num_layers: 6
    dropout: 0.1
    radio_map_channels: 5
    osm_map_channels: 5
    
  # Cross-Attention Fusion
  fusion:
    d_fusion: 512
    nhead: 8
    dropout: 0.1
    
  # Output Heads
  coarse_head:
    grid_size: 32  # 32x32 grid cells
    use_softmax: true
    dropout: 0.1
    
  fine_head:
    type: "heteroscedastic"  # "simple", "heteroscedastic", "gmm"
    d_hidden: 256
    dropout: 0.1
    num_gaussians: 3  # For GMM mode
    predict_uncertainty: true
    top_k: 5  # Number of top coarse predictions to refine

# Dataset Configuration (required by model)
dataset:
  zarr_path: "data/synthetic/dataset_20251224_134711.zarr"
  scene_extent: 856  # Scene size in meters (from Boulder scene)
  map_resolution: 1.0  # Meters per pixel
  normalize_features: true
  handle_missing_values: "mask"

# Training Configuration
training:
  # Data
  batch_size: 32  # Reduced for CPU training
  num_workers: 4  # Reduced for CPU
  pin_memory: false  # False for CPU training
    
  # Optimization
  optimizer: "adamw"
  learning_rate: 0.0001
  weight_decay: 0.01
  
  scheduler: "cosine_with_warmup"
  warmup_steps: 500
  
  num_epochs: 10  # Short training for testing
  gradient_clip: 1.0
  
  loss:
    coarse_weight: 4.0  # Increased to prioritize learning spatial structure
    fine_weight: 1.0
    position_weight: 0.3

# Infrastructure Configuration
infrastructure:
  accelerator: "cpu"
  devices: 1
  precision: "32"
  num_workers: 4
  
  checkpoint:
    monitor: "val_median_error"
    mode: "min"
    save_top_k: 3
    
  early_stopping:
    monitor: "val_median_error"
    patience: 10
    mode: "min"
    
  logging:
    log_every_n_steps: 50

# Weights & Biases Configuration
wandb:
  project: "transformer-ue-localization"
  entity: null  # Your W&B username/team
  name: null  # Auto-generated if null
  tags:
    - "transformer"
    - "map-conditioned"
    - "baseline"
  notes: "Baseline training run with dual map conditioning"
  save_code: true
  log_model: true

# Reproducibility
seed: 42
deterministic: false  # Set true for full reproducibility (slower)

# Experiment Tracking
experiment:
  name: "baseline_dual_map"
  version: 1
  output_dir: "experiments"
  checkpoint_dir: "checkpoints"
