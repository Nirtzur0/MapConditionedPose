# Milestone 3: Map-Conditioned Transformer Training Configuration

model:
  name: "MapConditionedTransformer"
  
  # Radio Encoder (Temporal Transformer)
  radio_encoder:
    type: "SetTransformer"
    num_cells: 6  # Number of cells in the dataset
    num_beams: 64
    d_model: 512
    nhead: 8  # Changed from num_heads
    num_layers: 8
    dropout: 0.1
    max_seq_len: 20
    rt_features_dim: 10
    phy_features_dim: 8
    mac_features_dim: 6
    
    # Token Embedding
    embedding:
      cell_id_embed_dim: 64
      beam_id_embed_dim: 32
      time_embed_dim: 64
      feature_dim: 256  # RT+PHY+MAC feature vector
      
    # Masking
    use_token_masking: true  # For missing measurements
    use_feature_masking: true  # For missing features within tokens
    
  # Map Encoder (Vision Transformer)
  map_encoder:
    use_e2_equivariant: true  # Use E2 equivariant vision transformer (NEW)
    num_group_elements: 8  # p4m group: 4 rotations x 2 reflections (NEW)
    img_size: 256
    patch_size: 16
    in_channels: 10
    d_model: 768
    nhead: 8
    num_layers: 6
    dropout: 0.1
    radio_map_channels: 5
    osm_map_channels: 5
    
  # Cross-Attention Fusion
  fusion:
    d_fusion: 512
    nhead: 8
    dropout: 0.1
    
  # Output Heads
  coarse_head:
    grid_size: 32  # 32x32 grid cells
    use_softmax: true
    dropout: 0.1
    
  fine_head:
    type: "heteroscedastic"  # "simple", "heteroscedastic", "gmm"
    d_hidden: 256
    dropout: 0.1
    num_gaussians: 3  # For GMM mode
    predict_uncertainty: true
    top_k: 5  # Number of top coarse predictions to refine

# Dataset Configuration (required by model)
dataset:
  zarr_paths:
    - "data/processed/dataset_paris.zarr"
    - "data/processed/dataset_nyc.zarr"
    - "data/processed/dataset_tokyo.zarr"
  scene_extent: 856  # Scene size in meters (from Boulder scene)
  map_resolution: 1.0  # Meters per pixel
  normalize_features: true
  handle_missing_values: "mask"

# Training Configuration
training:
  # Data
  batch_size: 32
  num_workers: 4
  pin_memory: false
    
  # Optimization
  optimizer: "adamw"
  learning_rate: 0.0001
  weight_decay: 0.01
  
  scheduler: "cosine_with_warmup"
  warmup_steps: 500
  
  num_epochs: 100
  gradient_clip: 1.0
  
  loss:
    coarse_weight: 1.0
    fine_weight: 1.0

# Infrastructure Configuration
infrastructure:
  accelerator: "auto"
  devices: 1
  precision: "16-mixed"
  num_workers: 4
  
  checkpoint:
    monitor: "val_median_error"
    mode: "min"
    save_top_k: 3
    
  logging:
    log_every_n_steps: 50
    
# Experiment Tracking
experiment:
  name: "diverse_training_run"
  version: 1
  output_dir: "experiments"
  checkpoint_dir: "checkpoints"

# Weights & Biases Configuration
wandb:
  project: "transformer-ue-localization"
  entity: null
  name: "diverse_training_run"
  tags:
    - "transformer"
    - "map-conditioned"
    - "diverse-data"
  notes: "Training run with combined data from Paris, NYC, and Tokyo."
  save_code: true
  log_model: true

# Reproducibility
seed: 42
deterministic: false
