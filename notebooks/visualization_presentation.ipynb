{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UE Localization Pipeline: Visualization Presentation\n",
        "\n",
        "This notebook generates high-quality visualizations for the UE Localization pipeline. The sections are organized as follows:\n",
        "\n",
        "1. **Setup & Data Loading** - Load trained model checkpoint and validation data\n",
        "2. **Input Features** - Table view of radio measurements (RT, PHY, MAC) used by the model\n",
        "3. **Map Layers** - 10-panel grid showing Radio Map (5 channels) and OSM Map (5 channels)\n",
        "4. **Model Predictions** - GMM heatmap overlay showing predicted vs ground truth positions\n",
        "5. **Evaluation Metrics** - Error statistics and distribution analysis (histogram, CDF, box plot)\n",
        "6. **Loss Decomposition** - Coarse heatmap and Top-K cell probability visualization\n",
        "7. **Fine Refinement** - Offset magnitudes, uncertainties, and mixture component ellipses\n",
        "8. **Physics Loss** - Differentiable bilinear resampling and gradient visualization\n",
        "9. **Position Refinement** - Inference-time optimization trajectory and improvement analysis\n",
        "10. **Summary** - Comprehensive dashboard with metrics, losses, and CDF comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "423fe258",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /home/ubuntu/projects/MapConditionedPose\n",
            "Current working directory: /home/ubuntu/projects/MapConditionedPose\n",
            "Changed to: /home/ubuntu/projects/MapConditionedPose\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "import importlib\n",
        "\n",
        "# Get the absolute path to project root\n",
        "project_root = Path(\"/home/ubuntu/projects/MapConditionedPose\")\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "\n",
        "# Change to project root\n",
        "os.chdir(project_root)\n",
        "print(f\"Changed to: {os.getcwd()}\")\n",
        "\n",
        "# Add src to path\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Force deep reload of all src modules by removing them from cache first\n",
        "to_remove = [key for key in sys.modules.keys() if key.startswith('src.')]\n",
        "for key in to_remove:\n",
        "    del sys.modules[key]\n",
        "\n",
        "from src.training import UELocalizationLightning\n",
        "# Note: collate_fn is now None (uses PyTorch default) for LMDB dataset\n",
        "\n",
        "# Set aesthetic style\n",
        "plt.style.use('seaborn-v0_8-paper')\n",
        "sns.set_context(\"talk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Model and Data\n",
        "\n",
        "We load the trained model from a checkpoint. We patch the configuration to point to a valid dataset for visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0966a318",
      "metadata": {},
      "source": [
        "## 2. Input Features Overview\n",
        "\n",
        "The RadioEncoder processes multi-source temporal measurement sequences from cellular network layers. Below is a comprehensive breakdown of all input features, their shapes, and their role in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using fallback LMDB dataset: data/processed/dataset.lmdb\n",
            "Warning: Checkpoint checkpoints/quick_test/last.ckpt not found.\n",
            "Warning: Config checkpoints/quick_test/training_config.yaml not found.\n",
            "Loading model from: checkpoints/quick_test/last.ckpt\n",
            "Failed to load with embedded config: [Errno 2] No such file or directory: '/home/ubuntu/projects/MapConditionedPose/checkpoints/quick_test/last.ckpt'\n",
            "\n",
            "Trying with external config...\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'checkpoints/quick_test/training_config.yaml'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# First try loading without providing config (uses checkpoint's hyperparameters)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     model = \u001b[43mUELocalizationLightning\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Allow minor mismatches\u001b[39;49;00m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\u2713 Loaded model using checkpoint\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms embedded config\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/MapConditionedPose/.venv/lib/python3.12/site-packages/pytorch_lightning/utilities/model_helpers.py:130\u001b[39m, in \u001b[36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    127\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_type.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.method.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` cannot be called on an instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/MapConditionedPose/.venv/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1781\u001b[39m, in \u001b[36mLightningModule.load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, weights_only, **kwargs)\u001b[39m\n\u001b[32m   1696\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[32m   1697\u001b[39m \u001b[33;03mpassed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[32m   1698\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1779\u001b[39m \n\u001b[32m   1780\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1781\u001b[39m loaded = \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1790\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/MapConditionedPose/.venv/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:65\u001b[39m, in \u001b[36m_load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, weights_only, **kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     checkpoint = \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/MapConditionedPose/.venv/lib/python3.12/site-packages/lightning_fabric/utilities/cloud_io.py:72\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(path_or_url, map_location, weights_only)\u001b[39m\n\u001b[32m     71\u001b[39m fs = get_filesystem(path_or_url)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.load(\n\u001b[32m     74\u001b[39m         f,\n\u001b[32m     75\u001b[39m         map_location=map_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     76\u001b[39m         weights_only=weights_only,\n\u001b[32m     77\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/MapConditionedPose/.venv/lib/python3.12/site-packages/fsspec/spec.py:1349\u001b[39m, in \u001b[36mAbstractFileSystem.open\u001b[39m\u001b[34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[39m\n\u001b[32m   1348\u001b[39m ac = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mautocommit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._intrans)\n\u001b[32m-> \u001b[39m\u001b[32m1349\u001b[39m f = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/MapConditionedPose/.venv/lib/python3.12/site-packages/fsspec/implementations/local.py:214\u001b[39m, in \u001b[36mLocalFileSystem._open\u001b[39m\u001b[34m(self, path, mode, block_size, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28mself\u001b[39m.makedirs(\u001b[38;5;28mself\u001b[39m._parent(path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/MapConditionedPose/.venv/lib/python3.12/site-packages/fsspec/implementations/local.py:391\u001b[39m, in \u001b[36mLocalFileOpener.__init__\u001b[39m\u001b[34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28mself\u001b[39m.blocksize = io.DEFAULT_BUFFER_SIZE\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/MapConditionedPose/.venv/lib/python3.12/site-packages/fsspec/implementations/local.py:396\u001b[39m, in \u001b[36mLocalFileOpener._open\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.autocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode:\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m     \u001b[38;5;28mself\u001b[39m.f = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compression:\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/ubuntu/projects/MapConditionedPose/checkpoints/quick_test/last.ckpt'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     47\u001b[39m TEMP_CONFIG_PATH = \u001b[33m\"\u001b[39m\u001b[33mnotebooks/temp_viz_config.yaml\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBASE_CONFIG_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     50\u001b[39m         config = yaml.safe_load(f)\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPatching dataset path in config to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/MapConditionedPose/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'checkpoints/quick_test/training_config.yaml'"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "CHECKPOINT_PATH = \"checkpoints/quick_test/last.ckpt\"\n",
        "BASE_CONFIG_PATH = \"checkpoints/quick_test/training_config.yaml\"\n",
        "\n",
        "# Find a valid LMDB dataset\n",
        "PROCESSED_DATA_DIR = Path(\"data/processed\")\n",
        "possible_datasets = sorted(list(PROCESSED_DATA_DIR.glob(\"dataset_*.lmdb\")))\n",
        "if possible_datasets:\n",
        "    DATASET_PATH = str(possible_datasets[-1]) # Use latest\n",
        "    print(f\"Using LMDB dataset: {DATASET_PATH}\")\n",
        "else:\n",
        "    # Fallback\n",
        "    DATASET_PATH = \"data/processed/dataset.lmdb\"\n",
        "    print(f\"Using fallback LMDB dataset: {DATASET_PATH}\")\n",
        "\n",
        "if not os.path.exists(CHECKPOINT_PATH):\n",
        "    print(f\"Warning: Checkpoint {CHECKPOINT_PATH} not found.\")\n",
        "    # Try to find any checkpoint\n",
        "    ckpts = list(Path(\"checkpoints\").rglob(\"*.ckpt\"))\n",
        "    if ckpts:\n",
        "        CHECKPOINT_PATH = str(ckpts[0])\n",
        "        print(f\"Fallback to: {CHECKPOINT_PATH}\")\n",
        "\n",
        "# Find a valid config file\n",
        "if not os.path.exists(BASE_CONFIG_PATH):\n",
        "    print(f\"Warning: Config {BASE_CONFIG_PATH} not found.\")\n",
        "    # Try to find any training config\n",
        "    configs = list(Path(\"checkpoints\").rglob(\"training_config.yaml\"))\n",
        "    if configs:\n",
        "        BASE_CONFIG_PATH = str(configs[0])\n",
        "        print(f\"Fallback to: {BASE_CONFIG_PATH}\")\n",
        "\n",
        "# Load Model - Try to use the checkpoint's embedded config  \n",
        "print(f\"Loading model from: {CHECKPOINT_PATH}\")\n",
        "try:\n",
        "    # First try loading without providing config (uses checkpoint's hyperparameters)\n",
        "    model = UELocalizationLightning.load_from_checkpoint(\n",
        "        CHECKPOINT_PATH,\n",
        "        strict=False # Allow minor mismatches\n",
        "    )\n",
        "    print(\"\u2713 Loaded model using checkpoint's embedded config\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load with embedded config: {e}\")\n",
        "    print(\"\\nTrying with external config...\")\n",
        "    \n",
        "    # Fallback: try with external config and patching\n",
        "    TEMP_CONFIG_PATH = \"notebooks/temp_viz_config.yaml\"\n",
        "    try:\n",
        "        with open(BASE_CONFIG_PATH, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "        \n",
        "        print(f\"Patching dataset path in config to: {DATASET_PATH}\")\n",
        "        # Update to LMDB configuration\n",
        "        config['dataset']['lmdb_path'] = DATASET_PATH\n",
        "        \n",
        "        # Remove old Zarr config if present\n",
        "        if 'train_zarr_paths' in config['dataset']:\n",
        "            del config['dataset']['train_zarr_paths']\n",
        "        if 'test_zarr_path' in config['dataset']:\n",
        "            del config['dataset']['test_zarr_path']\n",
        "        if 'val_zarr_path' in config['dataset']:\n",
        "            del config['dataset']['val_zarr_path']\n",
        "        \n",
        "        with open(TEMP_CONFIG_PATH, 'w') as f:\n",
        "            yaml.dump(config, f)\n",
        "\n",
        "        model = UELocalizationLightning.load_from_checkpoint(\n",
        "            CHECKPOINT_PATH, \n",
        "            config_path=TEMP_CONFIG_PATH,\n",
        "            strict=False\n",
        "        )\n",
        "        print(\"\u2713 Loaded model using external config\")\n",
        "    finally:\n",
        "        if os.path.exists(TEMP_CONFIG_PATH):\n",
        "            os.remove(TEMP_CONFIG_PATH)\n",
        "\n",
        "model.eval()\n",
        "model.cuda() if torch.cuda.is_available() else model.cpu()\n",
        "print(f\"Model device: {model.device}\")\n",
        "\n",
        "# Setup Data\n",
        "# We manually setup the dataloader from the config in the checkpoint\n",
        "val_loader = model.val_dataloader()\n",
        "\n",
        "# Collect multiple batches and find one with diverse positions\n",
        "print(\"Searching for a batch with diverse positions...\")\n",
        "batch_iter = iter(val_loader)\n",
        "best_batch = None\n",
        "best_diversity = 0\n",
        "\n",
        "for batch_idx in range(min(20, len(val_loader))):  # Check up to 20 batches\n",
        "    try:\n",
        "        batch = next(batch_iter)\n",
        "    except StopIteration:\n",
        "        break\n",
        "    \n",
        "    positions = batch['position'].numpy()\n",
        "    # Measure diversity as range in both X and Y\n",
        "    x_range = positions[:, 0].max() - positions[:, 0].min()\n",
        "    y_range = positions[:, 1].max() - positions[:, 1].min()\n",
        "    diversity = x_range + y_range\n",
        "    \n",
        "    if diversity > best_diversity:\n",
        "        best_diversity = diversity\n",
        "        best_batch = batch\n",
        "        best_batch_idx = batch_idx\n",
        "    \n",
        "    # If we found a batch with good diversity, use it\n",
        "    if diversity > 0.2:  # At least 20% of scene covered\n",
        "        print(f\"Found diverse batch at index {batch_idx}\")\n",
        "        break\n",
        "\n",
        "if best_batch is not None:\n",
        "    batch = best_batch\n",
        "    print(f\"Using batch {best_batch_idx} with diversity {best_diversity:.3f}\")\n",
        "else:\n",
        "    print(\"Warning: Could not find diverse batch, using last available\")\n",
        "\n",
        "# Move batch to device\n",
        "device = model.device\n",
        "for k, v in batch.items():\n",
        "    if isinstance(v, torch.Tensor):\n",
        "        batch[k] = v.to(device)\n",
        "    elif isinstance(v, dict):\n",
        "        for sub_k, sub_v in v.items():\n",
        "            if isinstance(sub_v, torch.Tensor):\n",
        "                v[sub_k] = sub_v.to(device)\n",
        "\n",
        "# Show position distribution in this batch\n",
        "positions = batch['position'].cpu().numpy()\n",
        "print(f\"\\nLoaded batch with {positions.shape[0]} samples\")\n",
        "print(f\"Position range in batch:\")\n",
        "print(f\"  X: [{positions[:,0].min():.3f}, {positions[:,0].max():.3f}] (range: {positions[:,0].max() - positions[:,0].min():.3f})\")\n",
        "print(f\"  Y: [{positions[:,1].min():.3f}, {positions[:,1].max():.3f}] (range: {positions[:,1].max() - positions[:,1].min():.3f})\")\n",
        "\n",
        "# Print individual sample positions\n",
        "print(f\"\\nSample positions:\")\n",
        "for i, pos in enumerate(positions):\n",
        "    print(f\"  Sample {i}: ({pos[0]:.4f}, {pos[1]:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8602199d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display real input features from a sample in the batch\n",
        "sample_idx = 0\n",
        "measurements = batch['measurements']\n",
        "\n",
        "# Get actual dimensions from the data\n",
        "B = measurements['cell_ids'].shape[0]\n",
        "L = measurements['cell_ids'].shape[1]\n",
        "rt_dim = measurements['rt_features'].shape[-1]\n",
        "phy_dim = measurements['phy_features'].shape[-1]\n",
        "mac_dim = measurements['mac_features'].shape[-1]\n",
        "\n",
        "# Check if CFR is available\n",
        "has_cfr = 'cfr_magnitude' in measurements and measurements['cfr_magnitude'] is not None\n",
        "if has_cfr:\n",
        "    cfr_shape = measurements['cfr_magnitude'].shape\n",
        "    cfr_cells, cfr_subcarriers = cfr_shape[-2], cfr_shape[-1]\n",
        "else:\n",
        "    cfr_cells, cfr_subcarriers = 8, 64\n",
        "\n",
        "# Get sequence length for the sample (count valid measurements)\n",
        "mask = measurements['mask'][sample_idx]\n",
        "valid_seq_len = mask.sum().item()\n",
        "\n",
        "print(\"=\" * 120)\n",
        "print(\"RADIO ENCODER INPUT FEATURES - REAL DATA SAMPLE\")\n",
        "print(\"=\" * 120)\n",
        "print(f\"\\nBatch Configuration:\")\n",
        "print(f\"  \u2022 Batch Size: {B}\")\n",
        "print(f\"  \u2022 Max Sequence Length: {L}\")\n",
        "print(f\"  \u2022 Valid Measurements in Sample {sample_idx}: {valid_seq_len}\")\n",
        "print(f\"  \u2022 Model Hidden Dimension: 512\")\n",
        "print(f\"  \u2022 Embedding Dimension (d/4): 128\")\n",
        "print(f\"\\n\")\n",
        "\n",
        "# Create detailed feature table with REAL VALUES\n",
        "print(\"\u250c\" + \"\u2500\" * 118 + \"\u2510\")\n",
        "print(\"\u2502 EMBEDDINGS (Categorical \u2192 Learned 128-dim vectors)                                                            \u2502\")\n",
        "print(\"\u251c\" + \"\u2500\" * 118 + \"\u2524\")\n",
        "\n",
        "# Show first few timesteps\n",
        "timesteps_to_show = min(3, valid_seq_len)\n",
        "for t in range(timesteps_to_show):\n",
        "    cell_id = measurements['cell_ids'][sample_idx, t].item()\n",
        "    beam_id = measurements['beam_ids'][sample_idx, t].item()\n",
        "    timestamp = measurements['timestamps'][sample_idx, t].item()\n",
        "    \n",
        "    print(f\"\u2502 Timestep {t}: Cell ID = {cell_id:<6} | Beam ID = {beam_id:<6} | Timestamp = {timestamp:>8.3f}                        \u2502\")\n",
        "\n",
        "print(\"\u2502   \u2192 Each embedded to 128-dim vectors via learned lookup tables                                                \u2502\")\n",
        "print(\"\u2502   \u2192 Timestamp uses sinusoidal positional encoding: sin/cos of scaled time                                     \u2502\")\n",
        "print(\"\u2514\" + \"\u2500\" * 118 + \"\u2518\")\n",
        "\n",
        "print()\n",
        "print(\"\u250c\" + \"\u2500\" * 118 + \"\u2510\")\n",
        "print(f\"\u2502 RT LAYER FEATURES ({rt_dim} features from Sionna Ray-Tracing) - First valid measurement                               \u2502\")\n",
        "print(\"\u251c\" + \"\u2500\" * 118 + \"\u2524\")\n",
        "\n",
        "rt_feature_names = [\n",
        "    'Path Gains', 'Path Delays', 'AoA Azimuth', 'AoA Elevation', \n",
        "    'AoD Azimuth', 'AoD Elevation', 'Doppler', 'RMS Delay Spread',\n",
        "    'K-Factor', 'Num Paths', 'ToA', 'Is NLoS', 'RMS Angular Spread'\n",
        "]\n",
        "rt_values = measurements['rt_features'][sample_idx, 0, :rt_dim].cpu().numpy()\n",
        "for i, (name, val) in enumerate(zip(rt_feature_names[:rt_dim], rt_values)):\n",
        "    print(f\"\u2502 [{i:2d}] {name:<25} = {val:>10.4f}                                                           \u2502\")\n",
        "print(\"\u2502   \u2192 All 13 features projected to 128-dim via Linear(13\u2192128) + LayerNorm                                       \u2502\")\n",
        "print(\"\u2514\" + \"\u2500\" * 118 + \"\u2518\")\n",
        "\n",
        "print()\n",
        "print(\"\u250c\" + \"\u2500\" * 118 + \"\u2510\")\n",
        "print(f\"\u2502 PHY LAYER FEATURES ({phy_dim} features from Physical Layer - FAPI) - First valid measurement                         \u2502\")\n",
        "print(\"\u251c\" + \"\u2500\" * 118 + \"\u2524\")\n",
        "\n",
        "phy_feature_names = [\n",
        "    'RSRP (dBm)', 'RSRQ', 'SINR (dB)', 'CQI', \n",
        "    'RI', 'PMI', 'Capacity (Mbps)', 'Condition Number'\n",
        "]\n",
        "phy_values = measurements['phy_features'][sample_idx, 0, :phy_dim].cpu().numpy()\n",
        "for i, (name, val) in enumerate(zip(phy_feature_names[:phy_dim], phy_values)):\n",
        "    print(f\"\u2502 [{i:2d}] {name:<25} = {val:>10.4f}                                                           \u2502\")\n",
        "print(\"\u2502   \u2192 All 8 features projected to 128-dim via Linear(8\u2192128) + LayerNorm                                         \u2502\")\n",
        "print(\"\u2514\" + \"\u2500\" * 118 + \"\u2518\")\n",
        "\n",
        "print()\n",
        "print(\"\u250c\" + \"\u2500\" * 118 + \"\u2510\")\n",
        "print(f\"\u2502 MAC LAYER FEATURES ({mac_dim} features from MAC/RRC) - First valid measurement                                        \u2502\")\n",
        "print(\"\u251c\" + \"\u2500\" * 118 + \"\u2524\")\n",
        "\n",
        "mac_feature_names = [\n",
        "    'Serving Cell ID', 'Timing Advance', 'Power Headroom (dB)', \n",
        "    'Throughput (Mbps)', 'BLER (%)'\n",
        "]\n",
        "mac_values = measurements['mac_features'][sample_idx, 0, :mac_dim].cpu().numpy()\n",
        "for i, (name, val) in enumerate(zip(mac_feature_names[:mac_dim], mac_values)):\n",
        "    print(f\"\u2502 [{i:2d}] {name:<25} = {val:>10.4f}                                                           \u2502\")\n",
        "print(\"\u2502   \u2192 All 5 features projected to 128-dim via Linear(5\u2192128) + LayerNorm                                         \u2502\")\n",
        "print(\"\u2514\" + \"\u2500\" * 118 + \"\u2518\")\n",
        "\n",
        "print()\n",
        "if has_cfr:\n",
        "    print(\"\u250c\" + \"\u2500\" * 118 + \"\u2510\")\n",
        "    print(f\"\u2502 CFR (Channel Frequency Response from DMRS) [{cfr_cells}\u00d7{cfr_subcarriers}]                                                     \u2502\")\n",
        "    print(\"\u251c\" + \"\u2500\" * 118 + \"\u2524\")\n",
        "    cfr_data = measurements['cfr_magnitude'][sample_idx].cpu().numpy()\n",
        "    print(f\"\u2502 Shape: [{cfr_cells} cells, {cfr_subcarriers} subcarriers]                                                                      \u2502\")\n",
        "    print(f\"\u2502 Mean magnitude: {cfr_data.mean():>8.4f}  |  Std: {cfr_data.std():>8.4f}  |  Max: {cfr_data.max():>8.4f}                              \u2502\")\n",
        "    print(f\"\u2502 Sample values (first 8 subcarriers of cell 0): {str(cfr_data[0, :8]):<45}  \u2502\")\n",
        "    print(\"\u2502   \u2192 Encoded via 1D convolutions along frequency axis \u2192 Global pooling \u2192 Linear projection to 128-dim       \u2502\")\n",
        "    print(\"\u2514\" + \"\u2500\" * 118 + \"\u2518\")\n",
        "    print()\n",
        "else:\n",
        "    print(\"\u250c\" + \"\u2500\" * 118 + \"\u2510\")\n",
        "    print(\"\u2502 CFR (Channel Frequency Response) - NOT AVAILABLE IN THIS DATASET                                              \u2502\")\n",
        "    print(\"\u2514\" + \"\u2500\" * 118 + \"\u2518\")\n",
        "    print()\n",
        "\n",
        "print(\"\u250c\" + \"\u2500\" * 118 + \"\u2510\")\n",
        "print(\"\u2502 ENCODER ARCHITECTURE - DATA FLOW                                                                               \u2502\")\n",
        "print(\"\u251c\" + \"\u2500\" * 118 + \"\u2524\")\n",
        "print(f\"\u2502 1. Embeddings + Feature Projections:                                                                          \u2502\")\n",
        "print(f\"\u2502    \u2022 Cell ID Embedding:      [B={B}, L={valid_seq_len}] \u2192 [B, L, 128]                                                 \u2502\")\n",
        "print(f\"\u2502    \u2022 Beam ID Embedding:      [B={B}, L={valid_seq_len}] \u2192 [B, L, 128]                                                 \u2502\")\n",
        "print(f\"\u2502    \u2022 Timestamp Encoding:     [B={B}, L={valid_seq_len}] \u2192 [B, L, 128]  (sinusoidal)                                   \u2502\")\n",
        "print(f\"\u2502    \u2022 RT Features Projection: [B={B}, L={valid_seq_len}, {rt_dim}] \u2192 [B, L, 128]                                            \u2502\")\n",
        "print(f\"\u2502    \u2022 PHY Features Projection:[B={B}, L={valid_seq_len}, {phy_dim}] \u2192 [B, L, 128]                                             \u2502\")\n",
        "print(f\"\u2502    \u2022 MAC Features Projection:[B={B}, L={valid_seq_len}, {mac_dim}] \u2192 [B, L, 128]                                              \u2502\")\n",
        "if has_cfr:\n",
        "    print(f\"\u2502    \u2022 CFR Encoding:           [B={B}, {cfr_cells}, {cfr_subcarriers}] \u2192 [B, 128] (broadcast to [B, L, 128])                          \u2502\")\n",
        "    num_streams = 7\n",
        "else:\n",
        "    num_streams = 6\n",
        "print(f\"\u2502                                                                                                                \u2502\")\n",
        "print(f\"\u2502 2. Concatenation:            {num_streams} streams \u2192 [B, L, {num_streams}\u00d7128={num_streams*128}]                                                    \u2502\")\n",
        "print(f\"\u2502                                                                                                                \u2502\")\n",
        "print(f\"\u2502 3. Linear Projection:        [B, L, {num_streams*128}] \u2192 [B, L, 512]                                                          \u2502\")\n",
        "print(f\"\u2502                                                                                                                \u2502\")\n",
        "print(f\"\u2502 4. Prepend CLS Token:        [B, L, 512] \u2192 [B, L+1, 512]                                                      \u2502\")\n",
        "print(f\"\u2502                                                                                                                \u2502\")\n",
        "print(f\"\u2502 5. Transformer Encoder:      6 layers, 8 heads, masked self-attention                                         \u2502\")\n",
        "print(f\"\u2502                              [B, L+1, 512] \u2192 [B, L+1, 512]                                                     \u2502\")\n",
        "print(f\"\u2502                                                                                                                \u2502\")\n",
        "print(f\"\u2502 6. Extract CLS Token:        [B, L+1, 512] \u2192 [B, 512]  \u2190 Final sequence embedding                             \u2502\")\n",
        "print(\"\u2514\" + \"\u2500\" * 118 + \"\u2518\")\n",
        "\n",
        "print()\n",
        "print(\"Summary:\")\n",
        "print(f\"  \u2022 Input: {rt_dim + phy_dim + mac_dim} raw features + 2 IDs + 1 timestamp\" + (\" + 1 CFR matrix\" if has_cfr else \"\"))\n",
        "print(f\"  \u2022 Processing: {num_streams} parallel embedding streams\")\n",
        "print(f\"  \u2022 Output: Single 512-dim vector representing the measurement sequence\")\n",
        "print(f\"  \u2022 Sequence length: Variable (1 to 20), this sample has {valid_seq_len} valid measurements\")\n",
        "print(\"=\" * 120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Complete Input Feature Table\n",
        "\n",
        "**All Radio Encoder Inputs (Normalized)**\n",
        "\n",
        "Displays **ALL** input features to the RadioEncoder for a single sample. Values are shown in **normalized form** (zero-mean, unit-variance) as fed to the neural network.\n",
        "\n",
        "### Feature Layers (30 total + IDs):\n",
        "\n",
        "| Layer | Features | Source |\n",
        "|-------|----------|--------|\n",
        "| **RT** (13-16) | Path Gains, Delays, AoA/AoD (Az/El), Doppler, RMS-DS, K-Factor, NumPaths, ToA, IsNLOS | Sionna Ray-Tracing |\n",
        "| **PHY** (8) | RSRP, RSRQ, SINR, CQI, RI, PMI, Capacity, Condition Number | L1 FAPI / PHY Layer |\n",
        "| **MAC** (5-6) | Serving Cell ID, Timing Advance, PHR, Throughput, BLER | MAC/RRC Layer |\n",
        "| **IDs** (3) | Cell ID, Beam ID, Timestamp | Categorical / Temporal |\n",
        "| **CFR** (optional) | Channel Frequency Response [Cells \u00d7 Subcarriers] | DMRS Channel Estimates |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete Input Feature Sequence Visualization\n",
        "# Shows ALL features from the dataset schema (both normalized and original scale)\n",
        "\n",
        "def visualize_all_features(batch, sample_idx=0, model=None):\n",
        "    \"\"\"Display ALL input features in structured tables matching the dataset schema.\"\"\"\n",
        "    measurements = batch['measurements']\n",
        "    \n",
        "    # Extract data for the sample\n",
        "    mask = measurements['mask'][sample_idx].cpu().numpy()\n",
        "    seq_len = int(mask.sum())  # Only take valid steps\n",
        "    \n",
        "    if seq_len == 0:\n",
        "        print(\"\u26a0\ufe0f No valid measurements in this sample!\")\n",
        "        return None\n",
        "    \n",
        "    # === FEATURE SCHEMAS (matching RadioLocalizationDataset) ===\n",
        "    RT_FEATURE_NAMES = [\n",
        "        ('Path Gains', 'dB'),           # 0\n",
        "        ('Path Delays', 'ns'),          # 1\n",
        "        ('AoA Azimuth', '\u00b0'),           # 2\n",
        "        ('AoA Elevation', '\u00b0'),         # 3\n",
        "        ('AoD Azimuth', '\u00b0'),           # 4\n",
        "        ('AoD Elevation', '\u00b0'),         # 5\n",
        "        ('Doppler', 'Hz'),              # 6\n",
        "        ('RMS Delay Spread', 'ns'),     # 7\n",
        "        ('K-Factor', 'dB'),             # 8\n",
        "        ('Num Paths', ''),              # 9\n",
        "        ('ToA', 's'),                   # 10\n",
        "        ('Is NLOS', ''),                # 11\n",
        "        ('RMS Angular Spread', '\u00b0'),    # 12\n",
        "    ]\n",
        "    \n",
        "    PHY_FEATURE_NAMES = [\n",
        "        ('RSRP', 'dBm'),                # 0 - Reference Signal Received Power\n",
        "        ('RSRQ', 'dB'),                 # 1 - Reference Signal Received Quality\n",
        "        ('SINR', 'dB'),                 # 2 - Signal-to-Interference-plus-Noise Ratio\n",
        "        ('CQI', ''),                    # 3 - Channel Quality Indicator (0-15)\n",
        "        ('RI', ''),                     # 4 - Rank Indicator (1-8)\n",
        "        ('PMI', ''),                    # 5 - Precoding Matrix Indicator\n",
        "        ('Capacity', 'Mbps'),           # 6 - Achievable capacity\n",
        "        ('Condition Number', ''),       # 7 - Channel matrix condition\n",
        "    ]\n",
        "    \n",
        "    MAC_FEATURE_NAMES = [\n",
        "        ('Serving Cell ID', ''),        # 0\n",
        "        ('Timing Advance', ''),         # 1\n",
        "        ('PHR', 'dB'),                  # 2 - Power Headroom Report\n",
        "        ('Throughput', 'Mbps'),         # 3\n",
        "        ('BLER', '%'),                  # 4 - Block Error Rate\n",
        "    ]\n",
        "    \n",
        "    # Get actual dimensions from data\n",
        "    rt_dim = measurements['rt_features'].shape[-1]\n",
        "    phy_dim = measurements['phy_features'].shape[-1]\n",
        "    mac_dim = measurements['mac_features'].shape[-1]\n",
        "    \n",
        "    print(\"=\" * 110)\n",
        "    print(f\"COMPLETE INPUT FEATURE SEQUENCE (Sample {sample_idx})\")\n",
        "    print(\"=\" * 110)\n",
        "    print(f\"Valid Timesteps: {seq_len} | RT Features: {rt_dim} | PHY Features: {phy_dim} | MAC Features: {mac_dim}\")\n",
        "    print()\n",
        "    print(\"\u26a0\ufe0f  NOTE: Values shown are NORMALIZED (zero-mean, unit-variance) for neural network training.\")\n",
        "    print(\"    Original scale values would require denormalization using stored statistics.\")\n",
        "    print()\n",
        "    \n",
        "    # === 1. IDENTIFIERS & TIMESTAMPS ===\n",
        "    print(\"\u250c\" + \"\u2500\" * 108 + \"\u2510\")\n",
        "    print(\"\u2502 1. IDENTIFIERS & TIMESTAMPS                                                                              \u2502\")\n",
        "    print(\"\u251c\" + \"\u2500\" * 108 + \"\u2524\")\n",
        "    \n",
        "    id_data = {\n",
        "        'Step': list(range(seq_len)),\n",
        "        'Timestamp': measurements['timestamps'][sample_idx, :seq_len].cpu().numpy(),\n",
        "        'Cell ID': measurements['cell_ids'][sample_idx, :seq_len].cpu().numpy(),\n",
        "        'Beam ID': measurements['beam_ids'][sample_idx, :seq_len].cpu().numpy(),\n",
        "    }\n",
        "    df_ids = pd.DataFrame(id_data)\n",
        "    display(df_ids.style.set_caption(\"Identifiers (Not Normalized)\"))\n",
        "    print(\"\u2514\" + \"\u2500\" * 108 + \"\u2518\")\n",
        "    print()\n",
        "    \n",
        "    # === 2. RT LAYER FEATURES ===\n",
        "    print(\"\u250c\" + \"\u2500\" * 108 + \"\u2510\")\n",
        "    print(f\"\u2502 2. RT LAYER FEATURES - Sionna Ray-Tracing [{rt_dim} features]                                              \u2502\")\n",
        "    print(\"\u2502    Source: Sionna channel model - path gains, delays, angles from multipath propagation                  \u2502\")\n",
        "    print(\"\u251c\" + \"\u2500\" * 108 + \"\u2524\")\n",
        "    \n",
        "    rt_values = measurements['rt_features'][sample_idx, :seq_len, :].cpu().numpy()\n",
        "    rt_data = {'Step': list(range(seq_len))}\n",
        "    \n",
        "    for i in range(min(rt_dim, len(RT_FEATURE_NAMES))):\n",
        "        name, unit = RT_FEATURE_NAMES[i]\n",
        "        col_name = f\"{name} ({unit})\" if unit else name\n",
        "        rt_data[col_name] = rt_values[:, i]\n",
        "    \n",
        "    # Add any extra features beyond schema\n",
        "    for i in range(len(RT_FEATURE_NAMES), rt_dim):\n",
        "        rt_data[f'RT[{i}]'] = rt_values[:, i]\n",
        "    \n",
        "    df_rt = pd.DataFrame(rt_data)\n",
        "    styled_rt = df_rt.style.background_gradient(cmap='RdYlGn', axis=0, subset=df_rt.columns[1:]).format(precision=4)\n",
        "    styled_rt = styled_rt.set_caption(\"RT Features (Normalized)\")\n",
        "    display(styled_rt)\n",
        "    print(\"\u2514\" + \"\u2500\" * 108 + \"\u2518\")\n",
        "    print()\n",
        "    \n",
        "    # === 3. PHY LAYER FEATURES ===\n",
        "    print(\"\u250c\" + \"\u2500\" * 108 + \"\u2510\")\n",
        "    print(f\"\u2502 3. PHY LAYER FEATURES - Physical Layer / FAPI [{phy_dim} features]                                        \u2502\")\n",
        "    print(\"\u2502    Source: L1 PHY layer measurements - signal quality indicators from UE                                 \u2502\")\n",
        "    print(\"\u251c\" + \"\u2500\" * 108 + \"\u2524\")\n",
        "    \n",
        "    phy_values = measurements['phy_features'][sample_idx, :seq_len, :].cpu().numpy()\n",
        "    phy_data = {'Step': list(range(seq_len))}\n",
        "    \n",
        "    for i in range(min(phy_dim, len(PHY_FEATURE_NAMES))):\n",
        "        name, unit = PHY_FEATURE_NAMES[i]\n",
        "        col_name = f\"{name} ({unit})\" if unit else name\n",
        "        phy_data[col_name] = phy_values[:, i]\n",
        "    \n",
        "    for i in range(len(PHY_FEATURE_NAMES), phy_dim):\n",
        "        phy_data[f'PHY[{i}]'] = phy_values[:, i]\n",
        "    \n",
        "    df_phy = pd.DataFrame(phy_data)\n",
        "    styled_phy = df_phy.style.background_gradient(cmap='Blues', axis=0, subset=df_phy.columns[1:]).format(precision=4)\n",
        "    styled_phy = styled_phy.set_caption(\"PHY Features (Normalized)\")\n",
        "    display(styled_phy)\n",
        "    print(\"\u2514\" + \"\u2500\" * 108 + \"\u2518\")\n",
        "    print()\n",
        "    \n",
        "    # === 4. MAC LAYER FEATURES ===\n",
        "    print(\"\u250c\" + \"\u2500\" * 108 + \"\u2510\")\n",
        "    print(f\"\u2502 4. MAC LAYER FEATURES - MAC/RRC Layer [{mac_dim} features]                                                 \u2502\")\n",
        "    print(\"\u2502    Source: Higher layer reports - timing advance, throughput, cell selection                             \u2502\")\n",
        "    print(\"\u251c\" + \"\u2500\" * 108 + \"\u2524\")\n",
        "    \n",
        "    mac_values = measurements['mac_features'][sample_idx, :seq_len, :].cpu().numpy()\n",
        "    mac_data = {'Step': list(range(seq_len))}\n",
        "    \n",
        "    for i in range(min(mac_dim, len(MAC_FEATURE_NAMES))):\n",
        "        name, unit = MAC_FEATURE_NAMES[i]\n",
        "        col_name = f\"{name} ({unit})\" if unit else name\n",
        "        mac_data[col_name] = mac_values[:, i]\n",
        "    \n",
        "    for i in range(len(MAC_FEATURE_NAMES), mac_dim):\n",
        "        mac_data[f'MAC[{i}]'] = mac_values[:, i]\n",
        "    \n",
        "    df_mac = pd.DataFrame(mac_data)\n",
        "    styled_mac = df_mac.style.background_gradient(cmap='Oranges', axis=0, subset=df_mac.columns[1:]).format(precision=4)\n",
        "    styled_mac = styled_mac.set_caption(\"MAC Features (Normalized)\")\n",
        "    display(styled_mac)\n",
        "    print(\"\u2514\" + \"\u2500\" * 108 + \"\u2518\")\n",
        "    print()\n",
        "    \n",
        "    # === 5. CFR (Channel Frequency Response) ===\n",
        "    has_cfr = 'cfr_magnitude' in measurements and measurements['cfr_magnitude'] is not None\n",
        "    if has_cfr:\n",
        "        cfr = measurements['cfr_magnitude'][sample_idx].cpu().numpy()\n",
        "        if cfr.sum() != 0:  # Check if CFR has actual data\n",
        "            print(\"\u250c\" + \"\u2500\" * 108 + \"\u2510\")\n",
        "            print(f\"\u2502 5. CFR - Channel Frequency Response [{cfr.shape[0]} cells \u00d7 {cfr.shape[1]} subcarriers]                              \u2502\")\n",
        "            print(\"\u2502    Source: Channel estimate from DMRS - encodes distance/multipath info across frequency                 \u2502\")\n",
        "            print(\"\u251c\" + \"\u2500\" * 108 + \"\u2524\")\n",
        "            \n",
        "            fig, ax = plt.subplots(figsize=(14, 3))\n",
        "            im = ax.imshow(cfr, aspect='auto', cmap='viridis')\n",
        "            ax.set_xlabel('Subcarrier Index')\n",
        "            ax.set_ylabel('Cell Index')\n",
        "            ax.set_title(f'CFR Magnitude | Mean: {cfr.mean():.4f} | Std: {cfr.std():.4f}')\n",
        "            plt.colorbar(im, ax=ax, label='Magnitude (Normalized)')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            print(\"\u2514\" + \"\u2500\" * 108 + \"\u2518\")\n",
        "        else:\n",
        "            print(\"\u2502 CFR: Shape available but values are zeros (placeholder)                                              \u2502\")\n",
        "    else:\n",
        "        print(\"\u250c\" + \"\u2500\" * 108 + \"\u2510\")\n",
        "        print(\"\u2502 5. CFR: Not available in this dataset                                                                  \u2502\")\n",
        "        print(\"\u2514\" + \"\u2500\" * 108 + \"\u2518\")\n",
        "    \n",
        "    # === SUMMARY TABLE ===\n",
        "    print()\n",
        "    print(\"\u250c\" + \"\u2500\" * 108 + \"\u2510\")\n",
        "    print(\"\u2502 FEATURE SUMMARY                                                                                          \u2502\")\n",
        "    print(\"\u251c\" + \"\u2500\" * 108 + \"\u2524\")\n",
        "    \n",
        "    summary_data = []\n",
        "    \n",
        "    # RT features summary\n",
        "    for i, (name, unit) in enumerate(RT_FEATURE_NAMES[:min(rt_dim, len(RT_FEATURE_NAMES))]):\n",
        "        vals = rt_values[:, i]\n",
        "        summary_data.append({\n",
        "            'Layer': 'RT',\n",
        "            'Index': i,\n",
        "            'Feature': name,\n",
        "            'Unit': unit,\n",
        "            'Min': vals.min(),\n",
        "            'Max': vals.max(),\n",
        "            'Mean': vals.mean(),\n",
        "            'Std': vals.std()\n",
        "        })\n",
        "    \n",
        "    # PHY features summary\n",
        "    for i, (name, unit) in enumerate(PHY_FEATURE_NAMES[:min(phy_dim, len(PHY_FEATURE_NAMES))]):\n",
        "        vals = phy_values[:, i]\n",
        "        summary_data.append({\n",
        "            'Layer': 'PHY',\n",
        "            'Index': i,\n",
        "            'Feature': name,\n",
        "            'Unit': unit,\n",
        "            'Min': vals.min(),\n",
        "            'Max': vals.max(),\n",
        "            'Mean': vals.mean(),\n",
        "            'Std': vals.std()\n",
        "        })\n",
        "    \n",
        "    # MAC features summary\n",
        "    for i, (name, unit) in enumerate(MAC_FEATURE_NAMES[:min(mac_dim, len(MAC_FEATURE_NAMES))]):\n",
        "        vals = mac_values[:, i]\n",
        "        summary_data.append({\n",
        "            'Layer': 'MAC',\n",
        "            'Index': i,\n",
        "            'Feature': name,\n",
        "            'Unit': unit,\n",
        "            'Min': vals.min(),\n",
        "            'Max': vals.max(),\n",
        "            'Mean': vals.mean(),\n",
        "            'Std': vals.std()\n",
        "        })\n",
        "    \n",
        "    df_summary = pd.DataFrame(summary_data)\n",
        "    styled_summary = df_summary.style.format({\n",
        "        'Min': '{:.4f}', 'Max': '{:.4f}', 'Mean': '{:.4f}', 'Std': '{:.4f}'\n",
        "    }).set_caption(\"All Features Summary (Normalized Values)\")\n",
        "    display(styled_summary)\n",
        "    \n",
        "    print(\"\u2514\" + \"\u2500\" * 108 + \"\u2518\")\n",
        "    print()\n",
        "    print(\"=\" * 110)\n",
        "    print(f\"TOTAL: {rt_dim + phy_dim + mac_dim} scalar features + 3 IDs per timestep\" + \n",
        "          (f\" + CFR [{cfr.shape[0]}\u00d7{cfr.shape[1]}]\" if has_cfr and cfr.sum() != 0 else \"\"))\n",
        "    print(\"=\" * 110)\n",
        "    \n",
        "    return {\n",
        "        'identifiers': df_ids,\n",
        "        'rt_features': df_rt,\n",
        "        'phy_features': df_phy, \n",
        "        'mac_features': df_mac,\n",
        "        'summary': df_summary\n",
        "    }\n",
        "\n",
        "# Run visualization\n",
        "feature_tables = visualize_all_features(batch, sample_idx=0, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualize Map Layers\n",
        "\n",
        "**Plot: 2\u00d75 Grid of Map Channels**\n",
        "\n",
        "Shows the 10-channel map input to the MapEncoder:\n",
        "\n",
        "**Radio Map (5 channels)**: Ray-tracing derived channel predictions\n",
        "- CH0: Path Loss, CH1: RSRP, CH2: Delay Spread, CH3: K-Factor, CH4: AoA\n",
        "\n",
        "**OSM Map (5 channels)**: OpenStreetMap-derived urban geometry\n",
        "- CH0: Building Height, CH1: Building Footprint, CH2: Roads, CH3: Distance Transform, CH4: Edges\n",
        "\n",
        "These multi-channel maps provide spatial context for the positioning task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from scipy.ndimage import binary_fill_holes, binary_dilation\n",
        "\n",
        "def _prettify_footprint(osm_map):\n",
        "    height = osm_map[0] if osm_map.shape[0] > 0 else None\n",
        "    if osm_map.shape[0] >= 5:\n",
        "        raw = osm_map[2]\n",
        "    elif osm_map.shape[0] >= 2:\n",
        "        raw = osm_map[1]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    mask = raw > 0.5\n",
        "    if height is not None and mask.mean() < 0.01:\n",
        "        mask = height > 0\n",
        "\n",
        "    mask = binary_dilation(mask, iterations=1)\n",
        "    mask = binary_fill_holes(mask)\n",
        "    return mask.astype(float)\n",
        "\n",
        "\n",
        "def visualize_maps(batch, sample_idx=0):\n",
        "    radio_map = batch['radio_map'][sample_idx].cpu().numpy()\n",
        "    osm_map = batch['osm_map'][sample_idx].cpu().numpy()\n",
        "    \n",
        "    # Radio Map Channels: ['Path Gain', 'ToA', 'SNR', 'SINR', 'Throughput']\n",
        "    # OSM Map Channels: ['Height', 'Material', 'Footprint', 'Road', 'Terrain']\n",
        "    # Note: Material (1), Road (3), Terrain (4) are often constant/empty but included for compat\n",
        "    \n",
        "    num_osm_channels = osm_map.shape[0]\n",
        "    footprint_idx = 2 if num_osm_channels >= 5 else (1 if num_osm_channels >= 2 else None)\n",
        "    pretty_footprint = _prettify_footprint(osm_map) if footprint_idx is not None else None\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "    \n",
        "    # Plot Radio Maps - use origin='lower' so (0,0) is bottom-left matching position coords\n",
        "    radio_titles = ['Path Gain', 'ToA', 'SNR', 'SINR', 'Throughput']\n",
        "    for i in range(5):\n",
        "        im = axes[0, i].imshow(radio_map[i], cmap='inferno', origin='lower')\n",
        "        axes[0, i].set_title(f\"Radio: {radio_titles[i]}\")\n",
        "        axes[0, i].axis('off')\n",
        "        plt.colorbar(im, ax=axes[0, i], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Plot OSM Maps (all 5 channels for backward compatibility)\n",
        "    osm_titles = ['Height', 'Material', 'Footprint', 'Road', 'Terrain']\n",
        "    for i in range(min(num_osm_channels, 5)):\n",
        "        if footprint_idx is not None and i == footprint_idx and pretty_footprint is not None:\n",
        "            im = axes[1, i].imshow(pretty_footprint, cmap='gray', origin='lower', vmin=0, vmax=1, interpolation='nearest')\n",
        "        else:\n",
        "            cmap = 'bone' if i in [0, 2] else 'viridis'\n",
        "            im = axes[1, i].imshow(osm_map[i], cmap=cmap, origin='lower')\n",
        "        axes[1, i].set_title(f\"OSM: {osm_titles[i]}\")\n",
        "        axes[1, i].axis('off')\n",
        "        plt.colorbar(im, ax=axes[1, i], fraction=0.046, pad=0.04)\n",
        "    \n",
        "    # Hide unused subplot axes if fewer channels\n",
        "    for i in range(num_osm_channels, 5):\n",
        "        axes[1, i].axis('off')\n",
        "        axes[1, i].set_title(\"(Not Used)\")\n",
        "    \n",
        "    plt.suptitle(f\"Map Layers (Sample {sample_idx})\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_maps(batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ede59056",
      "metadata": {},
      "source": [
        "### \ud83d\udcca Interpretation: Map Visualization\n",
        "\n",
        "**What to look for:**\n",
        "- **Radio Map Coverage**: Path gain should show clear signal decay from transmitters, with shadows behind buildings\n",
        "- **Consistent Patterns**: ToA, SNR, and SINR should correlate with path gain patterns\n",
        "- **OSM Map Quality**: Height map should clearly show building footprints; footprint channel should be binary (0 or 1)\n",
        "- **Alignment**: Radio and OSM maps should spatially align (same scene extent)\n",
        "\n",
        "**Common Issues:**\n",
        "- **All zeros in radio map**: Scene may not have loaded properly or transmitters are missing\n",
        "- **Uniform OSM channels**: Scene may have missing building data or incorrect material assignments\n",
        "- **Misalignment**: Check that coordinate transformations are consistent between radio and OSM rendering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ae5dff0",
      "metadata": {},
      "source": [
        "## 5. Sionna Native 3D Scene Visualization\n",
        "\n",
        "**Plot: Interactive 3D Scene with Radio Map Overlay**\n",
        "\n",
        "Uses Sionna's native rendering engine to visualize:\n",
        "- **3D Building Geometry**: The urban environment from OpenStreetMap\n",
        "- **Radio Map Coverage**: Path gain heatmap overlaid on the ground plane\n",
        "- **Transmitter Locations**: Base station positions in the scene\n",
        "- **UE Position**: Current sample's ground truth location\n",
        "\n",
        "> \u26a0\ufe0f **Note**: The 3D radio maps are rendered for **ALL scenes** during the pipeline.\n",
        "> See pre-computed renders in: `outputs/*/visualizations/3d/3d_visualizations/`\n",
        "> \n",
        "> This live visualization uses one scene at a time. Since samples don't store their source scene ID,\n",
        "> you can specify which scene to render using the `scene_name` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "904418c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_available_scenes():\n",
        "    \"\"\"List all available scenes in the data/scenes directory.\"\"\"\n",
        "    scenes_dir = Path(\"data/scenes\")\n",
        "    scene_files = sorted(scenes_dir.rglob(\"scene.xml\"))\n",
        "    scenes = {}\n",
        "    for sf in scene_files:\n",
        "        name = sf.parent.parent.name  # Get the city name (parent of scene_xxx folder)\n",
        "        scenes[name] = sf\n",
        "    return scenes\n",
        "\n",
        "def visualize_sionna_3d_scene(batch, sample_idx=0, scene_path=None, scene_name=None):\n",
        "    \"\"\"\n",
        "    Render a 3D visualization of the Sionna scene with radio map overlay.\n",
        "    \n",
        "    NOTE: Since samples in the dataset don't store their source scene_id,\n",
        "    you can specify which scene to visualize using the scene_name parameter.\n",
        "    The 3D radio maps ARE rendered for all scenes during the pipeline - \n",
        "    check outputs/*/visualizations/3d/ for the pre-computed renders.\n",
        "    \n",
        "    Args:\n",
        "        batch: Batch of data from the dataloader\n",
        "        sample_idx: Index of the sample within the batch\n",
        "        scene_path: Direct path to scene.xml (overrides scene_name)\n",
        "        scene_name: Name of scene to use (e.g., 'boulder_colorado', 'austin_texas', 'nyc_ny')\n",
        "    \"\"\"\n",
        "    import io\n",
        "    from PIL import Image\n",
        "    \n",
        "    try:\n",
        "        import sionna as sn\n",
        "        from sionna.rt import Camera, load_scene\n",
        "        \n",
        "        # List available scenes\n",
        "        available_scenes = list_available_scenes()\n",
        "        \n",
        "        # Try to find and load the scene\n",
        "        if scene_path is None:\n",
        "            if scene_name and scene_name in available_scenes:\n",
        "                scene_path = available_scenes[scene_name]\n",
        "                print(f\"Using specified scene: {scene_name}\")\n",
        "            else:\n",
        "                # Search recursively for scene.xml files\n",
        "                scenes_dir = Path(\"data/scenes\")\n",
        "                scene_files = sorted(list(scenes_dir.rglob(\"scene.xml\")))\n",
        "                if not scene_files:\n",
        "                    scene_files = list(scenes_dir.rglob(\"*.xml\"))\n",
        "                if scene_files:\n",
        "                    scene_path = scene_files[0]\n",
        "                    selected_scene = scene_path.parent.parent.name\n",
        "                    print(f\"\u26a0\ufe0f No scene specified. Defaulting to first available: {selected_scene}\")\n",
        "                    print(f\"\\nAvailable scenes: {list(available_scenes.keys())}\")\n",
        "                    print(f\"To use a different scene, call: visualize_sionna_3d_scene(batch, scene_name='<name>')\")\n",
        "                    print(f\"\\n\ud83d\udca1 TIP: Pre-rendered 3D radio maps for ALL scenes are in:\")\n",
        "                    print(f\"   outputs/*/visualizations/3d/3d_visualizations/\")\n",
        "                else:\n",
        "                    print(\"No scene files found in data/scenes/. Showing fallback visualization.\")\n",
        "                    return visualize_radio_map_3d_fallback(batch, sample_idx)\n",
        "        \n",
        "        # Load scene\n",
        "        print(f\"\\nLoading Sionna scene from: {scene_path}\")\n",
        "        scene = load_scene(str(scene_path))\n",
        "        \n",
        "        # Get scene bounds\n",
        "        try:\n",
        "            bbox = scene.mi_scene.bbox()\n",
        "            x_min, y_min, z_min = bbox.min.x, bbox.min.y, bbox.min.z\n",
        "            x_max, y_max, z_max = bbox.max.x, bbox.max.y, bbox.max.z\n",
        "            cx = (x_min + x_max) / 2\n",
        "            cy = (y_min + y_max) / 2\n",
        "            max_dim = max(x_max - x_min, y_max - y_min)\n",
        "            ground_z = z_min\n",
        "            print(f\"Scene bounds: ({x_min:.1f}, {y_min:.1f}) to ({x_max:.1f}, {y_max:.1f}), size: {max_dim:.1f}m\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not get scene bounds: {e}\")\n",
        "            cx, cy, ground_z = 0, 0, 0\n",
        "            max_dim = 500\n",
        "        \n",
        "        # Get true position from batch and convert to world coordinates\n",
        "        true_pos = batch['position'][sample_idx].cpu().numpy()\n",
        "        sample_extent = batch['sample_extent'][sample_idx].item() if 'sample_extent' in batch else 512.0\n",
        "        \n",
        "        # Convert normalized position to world coordinates (centered at scene center)\n",
        "        ue_x = cx + (true_pos[0] - 0.5) * sample_extent\n",
        "        ue_y = cy + (true_pos[1] - 0.5) * sample_extent\n",
        "        ue_z = ground_z + 1.5  # UE height\n",
        "        \n",
        "        print(f\"UE position (world coords): ({ue_x:.1f}, {ue_y:.1f}, {ue_z:.1f})\")\n",
        "        \n",
        "        # Add UE as receiver\n",
        "        rx = sn.rt.Receiver(\"UE\", position=[float(ue_x), float(ue_y), float(ue_z)])\n",
        "        scene.add(rx)\n",
        "        \n",
        "        # Setup camera - isometric view\n",
        "        iso_dist = max_dim * 0.8\n",
        "        cam_z = ground_z + max_dim * 0.6\n",
        "        \n",
        "        cam = Camera(\n",
        "            position=[float(cx - iso_dist), float(cy - iso_dist), float(cam_z)],\n",
        "            look_at=[float(cx), float(cy), float(ground_z)]\n",
        "        )\n",
        "        \n",
        "        # Ensure we have a transmitter\n",
        "        if len(scene.transmitters) == 0:\n",
        "            tx = sn.rt.Transmitter(\"TX_1\", position=[float(cx), float(cy), float(ground_z + 30)])\n",
        "            scene.add(tx)\n",
        "            print(f\"Added transmitter at ({cx:.1f}, {cy:.1f}, {ground_z + 30:.1f})\")\n",
        "        \n",
        "        scene.tx_array = sn.rt.PlanarArray(num_rows=1, num_cols=1, pattern=\"iso\", polarization=\"V\")\n",
        "        scene.rx_array = sn.rt.PlanarArray(num_rows=1, num_cols=1, pattern=\"iso\", polarization=\"V\")\n",
        "        \n",
        "        # Generate radio map for visualization\n",
        "        print(\"Generating radio map for visualization (this may take a moment)...\")\n",
        "        solver = sn.rt.RadioMapSolver()\n",
        "        \n",
        "        # Use smaller cell size for faster computation\n",
        "        map_size = min(sample_extent, max_dim)\n",
        "        cell_size = max(5.0, map_size / 100)  # At most 100x100 cells\n",
        "        \n",
        "        radio_map = solver(\n",
        "            scene,\n",
        "            center=[float(cx), float(cy), float(ground_z + 1.5)],\n",
        "            size=[float(map_size), float(map_size)],\n",
        "            cell_size=[cell_size, cell_size],\n",
        "            orientation=[0.0, 0.0, 0.0],\n",
        "            max_depth=3,  # Reduce for faster computation\n",
        "            diffraction=False  # Disable for speed\n",
        "        )\n",
        "        print(\"Radio map generated successfully!\")\n",
        "        \n",
        "        # Create figure for visualization\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "        \n",
        "        # 1. Render scene to file and display\n",
        "        ax = axes[0]\n",
        "        ax.set_title(\"3D Scene (Isometric View)\", fontsize=12)\n",
        "        try:\n",
        "            print(\"Rendering 3D scene...\")\n",
        "            # Use render_to_file with a temporary file\n",
        "            import tempfile\n",
        "            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:\n",
        "                tmp_path = tmp.name\n",
        "            \n",
        "            scene.render_to_file(\n",
        "                camera=cam,\n",
        "                filename=tmp_path,\n",
        "                resolution=(512, 384)\n",
        "            )\n",
        "            \n",
        "            # Load and display\n",
        "            img = plt.imread(tmp_path)\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "            \n",
        "            # Cleanup\n",
        "            os.unlink(tmp_path)\n",
        "            print(\"3D scene render successful!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Plain render failed: {e}\")\n",
        "            ax.text(0.5, 0.5, f\"3D Render unavailable\\n(Mitsuba/GPU required)\", ha='center', va='center', \n",
        "                    transform=ax.transAxes, fontsize=12)\n",
        "            ax.set_facecolor('lightgray')\n",
        "            ax.axis('off')\n",
        "        \n",
        "        # 2. Radio map as 2D heatmap (more reliable than 3D overlay)\n",
        "        ax = axes[1]\n",
        "        ax.set_title(\"Radio Map - Path Gain (dB)\", fontsize=12)\n",
        "        try:\n",
        "            print(\"Extracting radio map for visualization...\")\n",
        "            # Extract path gain from radio map\n",
        "            path_gain = radio_map.path_gain\n",
        "            if hasattr(path_gain, 'numpy'):\n",
        "                path_gain = path_gain.numpy()\n",
        "            \n",
        "            # Convert to dB\n",
        "            path_gain_db = 10 * np.log10(np.maximum(path_gain, 1e-12))\n",
        "            \n",
        "            # Take max over transmitters if multiple\n",
        "            if path_gain_db.ndim > 2:\n",
        "                path_gain_2d = np.max(path_gain_db, axis=0)\n",
        "            else:\n",
        "                path_gain_2d = path_gain_db\n",
        "            \n",
        "            im = ax.imshow(path_gain_2d, cmap='inferno', origin='lower', \n",
        "                          extent=[cx - map_size/2, cx + map_size/2, \n",
        "                                  cy - map_size/2, cy + map_size/2],\n",
        "                          vmin=-120, vmax=-50)\n",
        "            \n",
        "            # Mark UE position\n",
        "            ax.scatter([ue_x], [ue_y], c='lime', s=150, marker='*', \n",
        "                      edgecolors='black', linewidth=1.5, zorder=10, label='UE Position')\n",
        "            \n",
        "            # Mark TX position\n",
        "            ax.scatter([cx], [cy], c='red', s=100, marker='^', \n",
        "                      edgecolors='black', linewidth=1.5, zorder=10, label='Transmitter')\n",
        "            \n",
        "            ax.set_xlabel('X (meters)')\n",
        "            ax.set_ylabel('Y (meters)')\n",
        "            ax.legend(loc='upper right')\n",
        "            plt.colorbar(im, ax=ax, label='Path Gain (dB)')\n",
        "            print(\"Radio map visualization successful!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Radio map visualization failed: {e}\")\n",
        "            ax.text(0.5, 0.5, f\"Radio map extraction failed:\\n{str(e)[:60]}\", \n",
        "                   ha='center', va='center', transform=ax.transAxes, fontsize=10)\n",
        "            ax.axis('off')\n",
        "        \n",
        "        scene_name = Path(scene_path).parent.parent.name if scene_path else \"Unknown\"\n",
        "        plt.suptitle(f\"Sionna 3D Scene: {scene_name}\\nUE Position: ({ue_x:.1f}, {ue_y:.1f}) m | Scene Size: {max_dim:.0f}m\", fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Use display() for reliable inline output in Jupyter\n",
        "        from IPython.display import display\n",
        "        display(fig)\n",
        "        plt.close(fig)\n",
        "        \n",
        "        # Cleanup\n",
        "        try:\n",
        "            scene.remove(\"UE\")\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        return radio_map\n",
        "        \n",
        "    except ImportError as e:\n",
        "        print(f\"Sionna not available: {e}\")\n",
        "        print(\"Falling back to matplotlib 3D visualization...\")\n",
        "        return visualize_radio_map_3d_fallback(batch, sample_idx)\n",
        "    except Exception as e:\n",
        "        print(f\"Sionna 3D rendering failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"Falling back to matplotlib 3D visualization...\")\n",
        "        return visualize_radio_map_3d_fallback(batch, sample_idx)\n",
        "\n",
        "\n",
        "def visualize_radio_map_3d_fallback(batch, sample_idx=0):\n",
        "    \"\"\"\n",
        "    Fallback 3D visualization using matplotlib when Sionna rendering is unavailable.\n",
        "    Shows the radio map as a 3D surface plot with the OSM height map.\n",
        "    \"\"\"\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "    \n",
        "    radio_map = batch['radio_map'][sample_idx].cpu().numpy()\n",
        "    osm_map = batch['osm_map'][sample_idx].cpu().numpy()\n",
        "    true_pos = batch['position'][sample_idx].cpu().numpy()\n",
        "    \n",
        "    h, w = radio_map.shape[-2:]\n",
        "    x = np.linspace(0, 1, w)\n",
        "    y = np.linspace(0, 1, h)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    \n",
        "    # Normalize data for visualization\n",
        "    path_gain = radio_map[0]  # First channel is path gain\n",
        "    path_gain_norm = (path_gain - path_gain.min()) / (path_gain.max() - path_gain.min() + 1e-8)\n",
        "    \n",
        "    # Get building heights from OSM map (first channel)\n",
        "    heights = osm_map[0] if osm_map.shape[0] > 0 else np.zeros((h, w))\n",
        "    heights_norm = (heights - heights.min()) / (heights.max() - heights.min() + 1e-8)\n",
        "    \n",
        "    fig = plt.figure(figsize=(16, 6))\n",
        "    \n",
        "    # 1. 3D Surface plot of radio map\n",
        "    ax1 = fig.add_subplot(121, projection='3d')\n",
        "    surf = ax1.plot_surface(X, Y, path_gain_norm * 0.3, cmap='inferno', alpha=0.8, \n",
        "                            linewidth=0, antialiased=True)\n",
        "    ax1.scatter([true_pos[0]], [true_pos[1]], [0.35], c='lime', s=100, marker='*', \n",
        "                edgecolors='black', zorder=10, label='UE Position')\n",
        "    ax1.set_xlabel('X (normalized)')\n",
        "    ax1.set_ylabel('Y (normalized)')\n",
        "    ax1.set_zlabel('Path Gain (normalized)')\n",
        "    ax1.set_title('Radio Map 3D Surface', fontsize=12)\n",
        "    ax1.view_init(elev=30, azim=-60)\n",
        "    fig.colorbar(surf, ax=ax1, shrink=0.5, label='Path Gain')\n",
        "    \n",
        "    # 2. Combined height + radio map\n",
        "    ax2 = fig.add_subplot(122, projection='3d')\n",
        "    \n",
        "    # Plot building heights as wireframe\n",
        "    ax2.plot_wireframe(X, Y, heights_norm * 0.5, color='gray', alpha=0.3, linewidth=0.5)\n",
        "    \n",
        "    # Overlay radio map as colored surface at ground level\n",
        "    surf2 = ax2.plot_surface(X, Y, np.zeros_like(path_gain_norm), \n",
        "                             facecolors=plt.cm.inferno(path_gain_norm),\n",
        "                             alpha=0.7, linewidth=0)\n",
        "    \n",
        "    ax2.scatter([true_pos[0]], [true_pos[1]], [0.05], c='lime', s=100, marker='*', \n",
        "                edgecolors='black', zorder=10, label='UE Position')\n",
        "    ax2.set_xlabel('X (normalized)')\n",
        "    ax2.set_ylabel('Y (normalized)')\n",
        "    ax2.set_zlabel('Height (normalized)')\n",
        "    ax2.set_title('Buildings + Radio Coverage', fontsize=12)\n",
        "    ax2.view_init(elev=45, azim=-45)\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.suptitle(f\"3D Visualization (Matplotlib Fallback)\\nUE: ({true_pos[0]:.3f}, {true_pos[1]:.3f})\", \n",
        "                 fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Use display() for reliable inline output in Jupyter\n",
        "    from IPython.display import display\n",
        "    display(fig)\n",
        "    plt.close(fig)\n",
        "    \n",
        "    return None\n",
        "\n",
        "\n",
        "# List available scenes\n",
        "available = list_available_scenes()\n",
        "print(\"=\" * 60)\n",
        "print(\"SIONNA 3D SCENE VISUALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nAvailable scenes for visualization: {list(available.keys())}\")\n",
        "print(\"\\nTo visualize a specific scene, use:\")\n",
        "print(\"  visualize_sionna_3d_scene(batch, scene_name='austin_texas')\")\n",
        "print()\n",
        "\n",
        "# Run with default (first) scene\n",
        "sionna_radio_map = visualize_sionna_3d_scene(batch, sample_idx=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8fb256d",
      "metadata": {},
      "source": [
        "### \ud83d\udcca Interpretation: 3D Scene Visualization\n",
        "\n",
        "**What to look for:**\n",
        "- **Scene Geometry**: Buildings should be clearly visible with proper heights and shapes\n",
        "- **Transmitter Placement**: Base stations should be strategically positioned (rooftops or elevated)\n",
        "- **Radio Coverage**: Path gain heatmap should show realistic propagation patterns (stronger near TXs, shadowing behind buildings)\n",
        "- **UE Position**: Ground truth location should be within the scene boundaries\n",
        "\n",
        "**Common Issues:**\n",
        "- **Missing geometry**: If buildings don't appear, check scene loading or Sionna integration\n",
        "- **Uniform radio map**: May indicate failed ray tracing or missing transmitter power settings\n",
        "- **UE outside scene**: Check coordinate transformation between simulation and storage frames\n",
        "- **No visualization output**: Fallback to matplotlib 3D plot if Sionna native rendering is unavailable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Predictions vs Ground Truth\n",
        "\n",
        "**Plot: GMM Heatmap Overlay**\n",
        "\n",
        "Visualizes the model's Gaussian Mixture Model prediction:\n",
        "- **Background**: OSM map layers (Height=R, Footprint=G, Road+Terrain=B)\n",
        "- **Overlay**: Turbo colormap showing GMM probability density\n",
        "- **Green Star**: Ground truth position\n",
        "- **Red X**: Predicted position (weighted mean of mixture)\n",
        "- **Dashed Circle**: Uncertainty estimate (\u03c3 from top component)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from scipy.ndimage import binary_fill_holes, binary_dilation\n",
        "\n",
        "def _prettify_footprint(osm_map):\n",
        "    height = osm_map[0] if osm_map.shape[0] > 0 else None\n",
        "    if osm_map.shape[0] >= 5:\n",
        "        raw = osm_map[2]\n",
        "    elif osm_map.shape[0] >= 2:\n",
        "        raw = osm_map[1]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    mask = raw > 0.5\n",
        "    if height is not None and mask.mean() < 0.01:\n",
        "        mask = height > 0\n",
        "\n",
        "    mask = binary_dilation(mask, iterations=1)\n",
        "    mask = binary_fill_holes(mask)\n",
        "    return mask.astype(float)\n",
        "\n",
        "\n",
        "def render_prediction(model, batch, sample_idx=0):\n",
        "    from matplotlib.patches import Ellipse\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Use the model's internal model (UELocalizationModel) directly\n",
        "        outputs = model.model(batch['measurements'], batch['radio_map'], batch['osm_map'])\n",
        "    \n",
        "    # Extract outputs for sample\n",
        "    top_k_indices = outputs['top_k_indices'][sample_idx]\n",
        "    top_k_probs = outputs['top_k_probs'][sample_idx]\n",
        "    fine_offsets = outputs['fine_offsets'][sample_idx]\n",
        "    fine_uncertainties = outputs['fine_uncertainties'][sample_idx]\n",
        "    pred_pos = outputs['predicted_position'][sample_idx].cpu().numpy()\n",
        "    true_pos = batch['position'][sample_idx].cpu().numpy()\n",
        "    \n",
        "    # Get map dimensions\n",
        "    h, w = batch['radio_map'].shape[-2:]\n",
        "    grid_size = model.model.grid_size\n",
        "    \n",
        "    # Render GMM Heatmap manually using model's coarse head\n",
        "    gmm_heatmap = np.zeros((h, w), dtype=np.float32)\n",
        "    \n",
        "    # For each top-K component, render a gaussian WITH anisotropic variance (elliptical)\n",
        "    for k in range(min(5, len(top_k_indices))):\n",
        "        cell_idx = top_k_indices[k].item()\n",
        "        prob = top_k_probs[k].item()\n",
        "        offset = fine_offsets[k].cpu().numpy()\n",
        "        sigma = fine_uncertainties[k].cpu().numpy()  # [\u03c3x, \u03c3y]\n",
        "        \n",
        "        # Cell center (in normalized coords [0, 1])\n",
        "        # cell_idx is row-major: cell_idx = row * grid_size + col\n",
        "        cell_col = cell_idx % grid_size\n",
        "        cell_row = cell_idx // grid_size\n",
        "        cell_x = (cell_col + 0.5) / grid_size\n",
        "        cell_y = (cell_row + 0.5) / grid_size\n",
        "        \n",
        "        # Add offset (clamped to valid range)\n",
        "        center_x = np.clip(cell_x + offset[0], 0, 1)\n",
        "        center_y = np.clip(cell_y + offset[1], 0, 1)\n",
        "        \n",
        "        # Convert to pixel coordinates (no Y-flip, using origin='lower')\n",
        "        cx_px = center_x * w\n",
        "        cy_px = center_y * h\n",
        "        \n",
        "        # Scale sigma to pixels - use actual sigma for anisotropic Gaussians\n",
        "        sigma_px_x = np.abs(sigma[0]) * w + 1e-3\n",
        "        sigma_px_y = np.abs(sigma[1]) * h + 1e-3\n",
        "        \n",
        "        # Create coordinate grids for origin='lower' \n",
        "        # Row 0 is at y=0 (bottom), row h-1 is at y=h (top)\n",
        "        y_grid, x_grid = np.ogrid[:h, :w]\n",
        "        \n",
        "        # Compute Gaussian with ANISOTROPIC variance (elliptical shape)\n",
        "        gauss = np.exp(-0.5 * ((x_grid - cx_px) / sigma_px_x)**2 \n",
        "                       -0.5 * ((y_grid - cy_px) / sigma_px_y)**2)\n",
        "        gmm_heatmap += prob * gauss\n",
        "    \n",
        "    # Normalize heatmap\n",
        "    if gmm_heatmap.max() > 0:\n",
        "        gmm_heatmap /= gmm_heatmap.max()\n",
        "    \n",
        "    # Prepare visualization\n",
        "    # Background: OSM channels - supports both 5-channel (old) and 2-channel (new) formats\n",
        "    osm_map = batch['osm_map'][sample_idx].cpu().numpy()\n",
        "    bg = np.zeros((h, w, 3))\n",
        "    bg[..., 0] = model._normalize_map(osm_map[0])  # Height (R)\n",
        "    pretty_footprint = _prettify_footprint(osm_map)\n",
        "    \n",
        "    # Handle both old (5-channel) and new (2-channel) OSM formats\n",
        "    if pretty_footprint is not None:\n",
        "        bg[..., 1] = pretty_footprint  # Footprint (G)\n",
        "        if osm_map.shape[0] >= 5:\n",
        "            bg[..., 2] = model._normalize_map(osm_map[3] + osm_map[4] * 0.5)  # Road+Terrain (B)\n",
        "        else:\n",
        "            bg[..., 2] = 0\n",
        "    elif osm_map.shape[0] >= 5:\n",
        "        # Old format: [Height, Material, Footprint, Road, Terrain]\n",
        "        bg[..., 1] = model._normalize_map(osm_map[2])  # Footprint (G)\n",
        "        bg[..., 2] = model._normalize_map(osm_map[3] + osm_map[4] * 0.5)  # Road+Terrain (B)\n",
        "    elif osm_map.shape[0] >= 2:\n",
        "        # New format: [Height, Footprint]\n",
        "        bg[..., 1] = model._normalize_map(osm_map[1])  # Footprint (G)\n",
        "        bg[..., 2] = 0\n",
        "    \n",
        "    # GMM Overlay\n",
        "    gmm_colored = plt.cm.turbo(gmm_heatmap)\n",
        "    gmm_colored[..., 3] = np.clip(gmm_heatmap * 0.8, 0, 0.8) # Alpha\n",
        "    \n",
        "    # Coordinates (convert normalized [0,1] to pixel coords)\n",
        "    # Using origin='lower' so no Y-flip needed\n",
        "    true_px = true_pos[0] * w\n",
        "    true_py = true_pos[1] * h\n",
        "    pred_px = pred_pos[0] * w\n",
        "    pred_py = pred_pos[1] * h\n",
        "    \n",
        "    # Compute error in meters (using actual scene extent if available)\n",
        "    scene_extent = batch['sample_extent'][sample_idx].item() if 'sample_extent' in batch else 512.0\n",
        "    error_m = np.linalg.norm((true_pos - pred_pos) * scene_extent)\n",
        "    \n",
        "    # Plot with origin='lower' for consistent coordinate system\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(bg, origin='lower', extent=[0, w, 0, h], interpolation='nearest')\n",
        "    ax.imshow(gmm_colored, origin='lower', extent=[0, w, 0, h])\n",
        "    \n",
        "    ax.scatter([true_px], [true_py], c='lime', s=150, marker='*', label='Ground Truth', edgecolors='black', zorder=10, linewidth=1.5)\n",
        "    ax.scatter([pred_px], [pred_py], c='red', s=100, marker='x', label='Prediction', linewidth=3, zorder=10)\n",
        "    \n",
        "    # Add uncertainty ELLIPSE (from first component) instead of circle\n",
        "    sigma = fine_uncertainties[0].cpu().numpy()\n",
        "    sigma_px_x = np.abs(sigma[0]) * w * 2  # 2x for 1-sigma ellipse (width = 2*sigma)\n",
        "    sigma_px_y = np.abs(sigma[1]) * h * 2  # 2x for 1-sigma ellipse (height = 2*sigma)\n",
        "    \n",
        "    # Print sigma values for debugging\n",
        "    print(f\"Predicted \u03c3x={sigma[0]:.4f}, \u03c3y={sigma[1]:.4f} (normalized coords)\")\n",
        "    print(f\"In pixels: \u03c3x={sigma_px_x/2:.1f}px, \u03c3y={sigma_px_y/2:.1f}px\")\n",
        "    print(f\"In meters: \u03c3x={np.abs(sigma[0])*scene_extent:.2f}m, \u03c3y={np.abs(sigma[1])*scene_extent:.2f}m\")\n",
        "    \n",
        "    # Create ELLIPSE with separate x and y radii\n",
        "    ellipse = Ellipse((pred_px, pred_py), width=sigma_px_x, height=sigma_px_y, \n",
        "                      color='red', fill=False, linestyle='--', \n",
        "                      label=f'Uncertainty (1\u03c3 ellipse)', linewidth=2)\n",
        "    ax.add_patch(ellipse)\n",
        "\n",
        "    ax.set_title(f\"Model Prediction vs Ground Truth (Sample {sample_idx})\\nError: {error_m:.2f} m | True Pos: ({true_pos[0]:.3f}, {true_pos[1]:.3f}) | Pred Pos: ({pred_pos[0]:.3f}, {pred_pos[1]:.3f})\", fontsize=12)\n",
        "    ax.legend(loc='upper right', fontsize=11)\n",
        "    ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return outputs\n",
        "\n",
        "# Use a sample with position more in the center for better visualization\n",
        "outputs = render_prediction(model, batch, sample_idx=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b624ab45",
      "metadata": {},
      "source": [
        "### Changes Made to Reduce Variance Predictions\n",
        "\n",
        "The following changes were made to the model to produce smaller, more confident uncertainty ellipses:\n",
        "\n",
        "1. **Ellipse Visualization (above cell)**: Changed from circle to proper 2D ellipse using separate \u03c3x and \u03c3y\n",
        "2. **Bounded Variance**: Modified `FineHead` in [src/models/heads.py](../src/models/heads.py) to use sigmoid with bounds:\n",
        "   - `sigma_min = 0.001` (~1m at 1km scene)\n",
        "   - `sigma_max = 0.1` (~100m at 1km scene)\n",
        "3. **Variance Regularization**: Added `variance_loss` term in [src/models/ue_localization_model.py](../src/models/ue_localization_model.py) that penalizes large variance predictions\n",
        "\n",
        "**Note**: The current checkpoint was trained without these changes. To see the effect, you need to retrain the model with the updated code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74030a61",
      "metadata": {},
      "source": [
        "### \ud83d\udcca Interpretation: Model Prediction Heatmap\n",
        "\n",
        "**What to look for:**\n",
        "- **Prediction Accuracy**: Red X (prediction) should be close to green star (ground truth)\n",
        "- **Confidence Distribution**: GMM heatmap (hot colors) should peak near the true location\n",
        "- **Multi-modal Predictions**: Multiple peaks may indicate ambiguity (e.g., symmetric building layout)\n",
        "- **Spatial Correlation**: High-confidence regions should align with areas of strong radio signal coverage\n",
        "\n",
        "**Performance Indicators:**\n",
        "- **Error < 10m**: Excellent - model has high confidence and accuracy\n",
        "- **Error 10-50m**: Good - typical urban localization performance\n",
        "- **Error > 50m**: Poor - check if model is trained properly or if there's a data issue\n",
        "- **Diffuse heatmap**: Low confidence - model is uncertain (may need more training or better features)\n",
        "\n",
        "**Common Issues:**\n",
        "- **Prediction far from truth**: Model may be underfitted, or test data distribution differs from training\n",
        "- **Uniform heatmap**: Model collapsed to outputting constant predictions (check training loss)\n",
        "- **Multiple peaks far apart**: Ambiguous scenario - consider adding more discriminative features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6095bb6d",
      "metadata": {},
      "source": [
        "## 6. Evaluation Metrics\n",
        "\n",
        "**Plots: Error Distribution Analysis (3 panels)**\n",
        "\n",
        "1. **Error Histogram** - Distribution of localization errors with mean/median lines\n",
        "2. **CDF Plot** - Cumulative distribution with 67th/90th percentile markers\n",
        "3. **Box Plot** - Statistical summary showing quartiles and outliers\n",
        "\n",
        "Key metrics computed: Mean, Median, RMSE, Std, 67th/90th/95th Percentiles, Min/Max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e51b74f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_evaluation_metrics(model, batch, default_scene_extent=512.0):\n",
        "    \"\"\"Compute comprehensive evaluation metrics for the batch.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        outputs = model.model(batch['measurements'], batch['radio_map'], batch['osm_map'])\n",
        "    \n",
        "    pred_pos = outputs['predicted_position'].cpu().numpy()  # [B, 2] normalized\n",
        "    true_pos = batch['position'].cpu().numpy()  # [B, 2] normalized\n",
        "    \n",
        "    # Get scene extent from batch if available, otherwise use default\n",
        "    if 'sample_extent' in batch:\n",
        "        scene_extent = batch['sample_extent'].cpu().numpy()  # [B]\n",
        "        # Use per-sample extent for error calculation\n",
        "        errors_m = np.linalg.norm((pred_pos - true_pos) * scene_extent[:, None], axis=1)\n",
        "        avg_extent = scene_extent.mean()\n",
        "    else:\n",
        "        scene_extent = default_scene_extent\n",
        "        errors_m = np.linalg.norm((pred_pos - true_pos) * scene_extent, axis=1)\n",
        "        avg_extent = scene_extent\n",
        "    \n",
        "    metrics = {\n",
        "        'Mean Error (m)': np.mean(errors_m),\n",
        "        'Median Error (m)': np.median(errors_m),\n",
        "        'RMSE (m)': np.sqrt(np.mean(errors_m**2)),\n",
        "        'Std Error (m)': np.std(errors_m),\n",
        "        '67th Percentile (m)': np.percentile(errors_m, 67),\n",
        "        '90th Percentile (m)': np.percentile(errors_m, 90),\n",
        "        '95th Percentile (m)': np.percentile(errors_m, 95),\n",
        "        'Max Error (m)': np.max(errors_m),\n",
        "        'Min Error (m)': np.min(errors_m),\n",
        "    }\n",
        "    \n",
        "    return metrics, errors_m, pred_pos, true_pos, avg_extent\n",
        "\n",
        "# Compute metrics\n",
        "metrics, errors_m, pred_pos, true_pos, scene_extent = compute_evaluation_metrics(model, batch)\n",
        "\n",
        "# Display metrics\n",
        "print(\"=\" * 50)\n",
        "print(\"EVALUATION METRICS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Scene Extent:            {scene_extent:.1f} m\")\n",
        "for key, value in metrics.items():\n",
        "    print(f\"  {key:25s}: {value:8.3f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Print per-sample details\n",
        "print(\"\\nPer-sample details:\")\n",
        "print(f\"{'Sample':>6} | {'True X':>8} {'True Y':>8} | {'Pred X':>8} {'Pred Y':>8} | {'Error (m)':>10}\")\n",
        "print(\"-\" * 65)\n",
        "for i in range(len(errors_m)):\n",
        "    print(f\"{i:>6} | {true_pos[i,0]:>8.4f} {true_pos[i,1]:>8.4f} | {pred_pos[i,0]:>8.4f} {pred_pos[i,1]:>8.4f} | {errors_m[i]:>10.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78b2ac1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_error_analysis(errors_m, title=\"Error Distribution\"):\n",
        "    \"\"\"Plot comprehensive error analysis visualizations.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "    \n",
        "    # 1. Histogram of errors\n",
        "    ax = axes[0]\n",
        "    ax.hist(errors_m, bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "    ax.axvline(np.median(errors_m), color='red', linestyle='--', linewidth=2, label=f'Median: {np.median(errors_m):.1f}m')\n",
        "    ax.axvline(np.mean(errors_m), color='orange', linestyle='-', linewidth=2, label=f'Mean: {np.mean(errors_m):.1f}m')\n",
        "    ax.set_xlabel('Localization Error (m)', fontsize=12)\n",
        "    ax.set_ylabel('Count', fontsize=12)\n",
        "    ax.set_title('Error Histogram', fontsize=14)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. CDF Plot\n",
        "    ax = axes[1]\n",
        "    sorted_errors = np.sort(errors_m)\n",
        "    cdf = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
        "    ax.plot(sorted_errors, cdf * 100, linewidth=2, color='steelblue')\n",
        "    ax.axhline(67, color='green', linestyle='--', alpha=0.7, label='67th percentile')\n",
        "    ax.axhline(90, color='orange', linestyle='--', alpha=0.7, label='90th percentile')\n",
        "    ax.set_xlabel('Localization Error (m)', fontsize=12)\n",
        "    ax.set_ylabel('CDF (%)', fontsize=12)\n",
        "    ax.set_title('Cumulative Distribution Function', fontsize=14)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_ylim([0, 100])\n",
        "    \n",
        "    # 3. Box Plot\n",
        "    ax = axes[2]\n",
        "    bp = ax.boxplot(errors_m, vert=True, widths=0.6, patch_artist=True)\n",
        "    bp['boxes'][0].set_facecolor('lightsteelblue')\n",
        "    bp['boxes'][0].set_edgecolor('steelblue')\n",
        "    ax.set_ylabel('Localization Error (m)', fontsize=12)\n",
        "    ax.set_title('Error Box Plot', fontsize=14)\n",
        "    ax.set_xticks([1])\n",
        "    ax.set_xticklabels(['All Samples'])\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add statistics text\n",
        "    stats_text = f\"Median: {np.median(errors_m):.1f}m\\nRMSE: {np.sqrt(np.mean(errors_m**2)):.1f}m\"\n",
        "    ax.text(1.3, np.median(errors_m), stats_text, fontsize=10, verticalalignment='center')\n",
        "    \n",
        "    plt.suptitle(title, fontsize=16, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot error analysis for the batch\n",
        "plot_error_analysis(errors_m, \"Batch Error Analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e91c251f",
      "metadata": {},
      "source": [
        "### \ud83d\udcca Interpretation: Error Distribution Analysis\n",
        "\n",
        "**What to look for:**\n",
        "- **Histogram Shape**: Should show a right-skewed distribution with most errors at low values\n",
        "- **Median vs Mean**: Median should be lower than mean (indicates outliers at high end)\n",
        "- **CDF Curve**: Should rise steeply at low errors (e.g., 50-70% of samples within 20-30m)\n",
        "- **Percentile Box Plot**: Most data should be in the lower quartiles, with outliers clearly visible\n",
        "\n",
        "**Performance Benchmarks:**\n",
        "- **Median < 20m**: Excellent performance for urban environments\n",
        "- **Median 20-50m**: Good performance, comparable to GPS in open areas\n",
        "- **Median > 50m**: Poor performance - model needs improvement or data quality check\n",
        "- **90th percentile < 100m**: Indicates consistent performance with few outliers\n",
        "\n",
        "**Common Issues:**\n",
        "- **Bimodal distribution**: Two distinct error clusters may indicate the model works well in some scenarios but fails in others\n",
        "- **High outliers (>200m)**: Check for data errors, coordinate system bugs, or edge cases in test set\n",
        "- **Flat CDF**: Model is not learning useful patterns - check training convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "962f5c1b",
      "metadata": {},
      "source": [
        "## 7. Coarse and Fine Loss Decomposition\n",
        "\n",
        "**Plots: Coarse Head Analysis (2 panels)**\n",
        "\n",
        "The model uses a **coarse-to-fine** approach:\n",
        "\n",
        "$$\\mathcal{L}_{\\text{total}} = \\lambda_{\\text{coarse}} \\mathcal{L}_{\\text{coarse}} + \\lambda_{\\text{fine}} \\mathcal{L}_{\\text{fine}}$$\n",
        "\n",
        "1. **Coarse Heatmap** - Grid cell probabilities with GT cell (green) and Top-K cells (colored borders)\n",
        "2. **Top-K Bar Chart** - Probability distribution over top candidate cells (green = correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93e4ff07",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_loss_breakdown(model, batch):\n",
        "    \"\"\"Compute and visualize the coarse and fine loss components.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        outputs = model.model(batch['measurements'], batch['radio_map'], batch['osm_map'])\n",
        "    \n",
        "    targets = {\n",
        "        'position': batch['position'],\n",
        "        'cell_grid': batch['cell_grid'],\n",
        "    }\n",
        "    \n",
        "    loss_weights = {'coarse_weight': 1.0, 'fine_weight': 1.0}\n",
        "    losses = model.model.compute_loss(outputs, targets, loss_weights)\n",
        "    \n",
        "    return losses, outputs\n",
        "\n",
        "losses, outputs = compute_loss_breakdown(model, batch)\n",
        "\n",
        "# Display loss breakdown\n",
        "print(\"=\" * 50)\n",
        "print(\"LOSS BREAKDOWN\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Total Loss:         {losses['loss'].item():.4f}\")\n",
        "print(f\"  Coarse Loss (CE):   {losses['coarse_loss'].item():.4f}\")\n",
        "print(f\"  Fine Loss (NLL):    {losses['fine_loss'].item():.4f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Visualize the coarse heatmap for a sample\n",
        "def visualize_coarse_heatmap(outputs, batch, sample_idx=0):\n",
        "    \"\"\"Visualize the coarse prediction heatmap and ground truth cell.\"\"\"\n",
        "    coarse_heatmap = outputs['coarse_heatmap'][sample_idx].cpu().numpy()\n",
        "    true_pos = batch['position'][sample_idx].cpu().numpy()\n",
        "    true_cell = batch['cell_grid'][sample_idx].item()\n",
        "    \n",
        "    grid_size = int(np.sqrt(coarse_heatmap.shape[0])) if coarse_heatmap.ndim == 1 else coarse_heatmap.shape[0]\n",
        "    \n",
        "    # Ensure heatmap is 2D\n",
        "    if coarse_heatmap.ndim == 1:\n",
        "        coarse_heatmap = coarse_heatmap.reshape(grid_size, grid_size)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # 1. Coarse heatmap with true position (using origin='lower' for consistency)\n",
        "    ax = axes[0]\n",
        "    im = ax.imshow(coarse_heatmap, cmap='hot', interpolation='nearest', origin='lower')\n",
        "    \n",
        "    # Mark ground truth cell - cell_idx = row * grid_size + col\n",
        "    # With origin='lower', row 0 is at the bottom\n",
        "    gt_row = true_cell // grid_size  # Y position (row)\n",
        "    gt_col = true_cell % grid_size   # X position (col)\n",
        "    rect = plt.Rectangle((gt_col - 0.5, gt_row - 0.5), 1, 1, fill=False, edgecolor='lime', linewidth=3, label='GT Cell')\n",
        "    ax.add_patch(rect)\n",
        "    \n",
        "    # Mark top-K cells\n",
        "    top_k_indices = outputs['top_k_indices'][sample_idx].cpu().numpy()\n",
        "    top_k_probs = outputs['top_k_probs'][sample_idx].cpu().numpy()\n",
        "    for k, (idx, prob) in enumerate(zip(top_k_indices[:3], top_k_probs[:3])):\n",
        "        k_row = idx // grid_size\n",
        "        k_col = idx % grid_size\n",
        "        color = ['cyan', 'yellow', 'orange'][k]\n",
        "        rect = plt.Rectangle((k_col - 0.5, k_row - 0.5), 1, 1, fill=False, edgecolor=color, linewidth=2, linestyle='--')\n",
        "        ax.add_patch(rect)\n",
        "    \n",
        "    # Add true position marker\n",
        "    ax.scatter(true_pos[0] * grid_size, true_pos[1] * grid_size, c='lime', s=100, marker='*', \n",
        "               edgecolors='black', zorder=10, label='True Pos')\n",
        "    \n",
        "    ax.set_xlabel('X (grid cells)')\n",
        "    ax.set_ylabel('Y (grid cells)')\n",
        "    ax.set_title(f'Coarse Prediction Heatmap (Sample {sample_idx})\\nGT Cell: {true_cell} (row={gt_row}, col={gt_col})', fontsize=12)\n",
        "    plt.colorbar(im, ax=ax, label='Probability')\n",
        "    ax.legend(loc='upper right')\n",
        "    \n",
        "    # 2. Top-K probabilities bar chart\n",
        "    ax = axes[1]\n",
        "    top_k = 5\n",
        "    indices = top_k_indices[:top_k]\n",
        "    probs = top_k_probs[:top_k]\n",
        "    \n",
        "    colors = ['green' if idx == true_cell else 'steelblue' for idx in indices]\n",
        "    bars = ax.bar(range(top_k), probs, color=colors, edgecolor='black')\n",
        "    ax.set_xticks(range(top_k))\n",
        "    ax.set_xticklabels([f'Cell {idx}' for idx in indices], rotation=45, ha='right')\n",
        "    ax.set_ylabel('Probability', fontsize=12)\n",
        "    ax.set_title('Top-K Cell Probabilities\\n(Green = Ground Truth)', fontsize=12)\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Annotate bars\n",
        "    for i, (bar, prob) in enumerate(zip(bars, probs)):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "                f'{prob:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_coarse_heatmap(outputs, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c6fa6a6",
      "metadata": {},
      "source": [
        "### \ud83d\udcca Interpretation: Coarse-to-Fine Loss Breakdown\n",
        "\n",
        "**What to look for:**\n",
        "- **Coarse Loss (CE)**: Should be relatively low (< 2.0) if model is classifying the correct grid cell\n",
        "- **Fine Loss (NLL)**: Negative log-likelihood for GMM - lower is better (typically 1.0-4.0)\n",
        "- **Loss Balance**: Both losses should contribute roughly equally; if one dominates, adjust loss weights\n",
        "- **Coarse Heatmap Peak**: Should be centered on or very close to the ground truth cell (green square)\n",
        "\n",
        "**Performance Indicators:**\n",
        "- **Coarse Loss < 1.0**: Model has high confidence in correct grid cell\n",
        "- **Coarse Loss > 3.0**: Model is uncertain or predicting wrong cell - check training progress\n",
        "- **Fine Loss < 2.0**: GMM prediction is confident and accurate within the cell\n",
        "- **Fine Loss > 5.0**: GMM is diffuse or misaligned - may need more training\n",
        "\n",
        "**Common Issues:**\n",
        "- **Coarse prediction far from GT**: Model may need longer training or better features\n",
        "- **Multiple bright cells**: Ambiguous scenario or model hasn't converged\n",
        "- **Uniform coarse heatmap**: Model collapsed - check learning rate or model initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c40ae433",
      "metadata": {},
      "source": [
        "## 8. Fine Refinement Head Analysis\n",
        "\n",
        "**Plots: Fine Head Outputs (3 panels)**\n",
        "\n",
        "The Fine Head outputs sub-cell refinements:\n",
        "\n",
        "$$\\hat{\\mathbf{y}} = \\text{CellCenter}(c^*) + \\Delta\\mathbf{y}$$\n",
        "\n",
        "1. **Offset Magnitudes** - Bar chart showing displacement from cell centers (meters)\n",
        "2. **Uncertainties (\u03c3x, \u03c3y)** - Predicted standard deviations per component\n",
        "3. **Mixture Components** - 2\u03c3 ellipses for each GMM component with ground truth star"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc943b35",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_fine_refinement(outputs, batch, sample_idx=0, scene_extent=512.0):\n",
        "    \"\"\"Visualize the fine refinement outputs (offsets and uncertainties).\"\"\"\n",
        "    fine_offsets = outputs['fine_offsets'][sample_idx].cpu().numpy()  # [K, 2]\n",
        "    fine_uncertainties = outputs['fine_uncertainties'][sample_idx].cpu().numpy()  # [K, 2]\n",
        "    top_k_indices = outputs['top_k_indices'][sample_idx].cpu().numpy()\n",
        "    top_k_probs = outputs['top_k_probs'][sample_idx].cpu().numpy()\n",
        "    true_pos = batch['position'][sample_idx].cpu().numpy()\n",
        "    pred_pos = outputs['predicted_position'][sample_idx].cpu().numpy()\n",
        "    \n",
        "    grid_size = model.model.grid_size\n",
        "    cell_size = 1.0 / grid_size  # Normalized cell size\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "    \n",
        "    # 1. Offset magnitudes for each component\n",
        "    ax = axes[0]\n",
        "    offset_magnitudes = np.linalg.norm(fine_offsets, axis=1) * scene_extent  # in meters\n",
        "    bars = ax.bar(range(len(offset_magnitudes)), offset_magnitudes, color='steelblue', edgecolor='black')\n",
        "    ax.set_xticks(range(len(offset_magnitudes)))\n",
        "    ax.set_xticklabels([f'K={i}' for i in range(len(offset_magnitudes))])\n",
        "    ax.set_ylabel('Offset Magnitude (m)', fontsize=12)\n",
        "    ax.set_title('Fine Offset Magnitudes per Component', fontsize=12)\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    ax.axhline(cell_size * scene_extent / 2, color='red', linestyle='--', label=f'Half Cell: {cell_size*scene_extent/2:.1f}m')\n",
        "    ax.legend()\n",
        "    \n",
        "    # 2. Uncertainty (sigma) for each component\n",
        "    ax = axes[1]\n",
        "    sigma_x = np.abs(fine_uncertainties[:, 0]) * scene_extent\n",
        "    sigma_y = np.abs(fine_uncertainties[:, 1]) * scene_extent\n",
        "    x = np.arange(len(sigma_x))\n",
        "    width = 0.35\n",
        "    ax.bar(x - width/2, sigma_x, width, label='\u03c3x', color='coral', edgecolor='black')\n",
        "    ax.bar(x + width/2, sigma_y, width, label='\u03c3y', color='teal', edgecolor='black')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels([f'K={i}' for i in range(len(sigma_x))])\n",
        "    ax.set_ylabel('Uncertainty \u03c3 (m)', fontsize=12)\n",
        "    ax.set_title('Predicted Uncertainties per Component', fontsize=12)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # 3. Visualize mixture components on a zoomed view\n",
        "    ax = axes[2]\n",
        "    \n",
        "    # Plot each mixture component as an ellipse\n",
        "    from matplotlib.patches import Ellipse\n",
        "    \n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(top_k_indices)))\n",
        "    for k, (idx, prob, offset, sigma, color) in enumerate(zip(top_k_indices, top_k_probs, fine_offsets, fine_uncertainties, colors)):\n",
        "        # Cell center (normalized) - idx = row * grid_size + col\n",
        "        col = idx % grid_size\n",
        "        row = idx // grid_size\n",
        "        cx = (col + 0.5) * cell_size  # X = column\n",
        "        cy = (row + 0.5) * cell_size  # Y = row\n",
        "        \n",
        "        # Apply offset\n",
        "        px = cx + offset[0]\n",
        "        py = cy + offset[1]\n",
        "        \n",
        "        # Draw ellipse (2-sigma)\n",
        "        ellipse = Ellipse((px, py), 2 * np.abs(sigma[0]), 2 * np.abs(sigma[1]), \n",
        "                          fill=False, edgecolor=color, linewidth=2, \n",
        "                          linestyle='-' if k == 0 else '--',\n",
        "                          label=f'K={k}: p={prob:.2f}')\n",
        "        ax.add_patch(ellipse)\n",
        "        ax.scatter(px, py, c=[color], s=50, marker='o', zorder=5)\n",
        "    \n",
        "    # Mark true position\n",
        "    ax.scatter(true_pos[0], true_pos[1], c='lime', s=150, marker='*', \n",
        "               edgecolors='black', zorder=10, label='Ground Truth')\n",
        "    \n",
        "    # Mark predicted position (weighted mean)\n",
        "    ax.scatter(pred_pos[0], pred_pos[1], c='red', s=100, marker='x', \n",
        "               linewidth=3, zorder=10, label='Prediction')\n",
        "    \n",
        "    ax.set_xlim([0, 1])\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xlabel('X (normalized)', fontsize=12)\n",
        "    ax.set_ylabel('Y (normalized)', fontsize=12)\n",
        "    ax.set_title('Mixture Components (2\u03c3 Ellipses)', fontsize=12)\n",
        "    ax.legend(loc='upper right', fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print diagnostic info\n",
        "    print(f\"True position: ({true_pos[0]:.4f}, {true_pos[1]:.4f})\")\n",
        "    print(f\"Predicted position: ({pred_pos[0]:.4f}, {pred_pos[1]:.4f})\")\n",
        "    print(f\"Error: {np.linalg.norm(true_pos - pred_pos) * scene_extent:.2f} m\")\n",
        "\n",
        "visualize_fine_refinement(outputs, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7dca137",
      "metadata": {},
      "source": [
        "### \ud83d\udcca Interpretation: Fine Refinement Analysis\n",
        "\n",
        "**What to look for:**\n",
        "- **Offset Magnitudes**: Should be within reasonable bounds (typically < half cell size, ~8m for 32x32 grid)\n",
        "- **Uncertainty Values**: Lower uncertainty (\u03c3) indicates higher confidence in the prediction\n",
        "- **Probability Distribution**: Top-K weights should be concentrated on 1-3 components, not uniformly spread\n",
        "- **Spatial Scatter**: Multiple candidate positions should cluster around the ground truth\n",
        "\n",
        "**Performance Indicators:**\n",
        "- **Dominant component (weight > 0.5)**: Model has high confidence in a single location\n",
        "- **Offset < 5m**: Precise refinement within the grid cell\n",
        "- **Low \u03c3 (< 5m)**: Model is confident in the exact position\n",
        "- **Scatter close to GT**: All top-K candidates near the true location indicates consistent prediction\n",
        "\n",
        "**Common Issues:**\n",
        "- **Large offsets (>20m)**: May indicate coordinate system mismatch or model instability\n",
        "- **High uncertainty (\u03c3 > 20m)**: Model is not confident - may need better features or more training\n",
        "- **Uniform weights**: Model unable to discriminate between candidates (check if coarse prediction is correct)\n",
        "- **Scattered candidates far from GT**: Model predictions are inconsistent - check data quality or model convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09f5d5dd",
      "metadata": {},
      "source": [
        "## 9. Physics Loss & Differentiable Bilinear Resampling\n",
        "\n",
        "**Plots: Bilinear Sampling Visualization (3 panels)**\n",
        "\n",
        "The Physics Loss enforces consistency using differentiable interpolation:\n",
        "\n",
        "$$\\mathcal{L}_{\\text{phys}} = \\sum_{f \\in \\mathcal{F}} w_f \\left\\| m_f^{\\text{obs}} - R_f(\\hat{\\mathbf{x}}) \\right\\|^2$$\n",
        "\n",
        "1. **Original Radio Map** - Path Gain channel with true/predicted positions marked\n",
        "2. **Bilinear Resampled** - 50\u00d750 grid showing interpolated values\n",
        "3. **Gradient Magnitude** - |\u2207f| with quiver arrows showing gradient direction\n",
        "\n",
        "This enables backpropagation through the position \u2192 feature lookup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4004a3ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.physics_loss import differentiable_lookup, normalize_coords, PhysicsLoss, PhysicsLossConfig\n",
        "\n",
        "def demonstrate_bilinear_resampling(batch, sample_idx=0):\n",
        "    \"\"\"Demonstrate differentiable bilinear interpolation from radio maps.\"\"\"\n",
        "    radio_map = batch['radio_map'][sample_idx:sample_idx+1]  # [1, C, H, W]\n",
        "    true_pos = batch['position'][sample_idx:sample_idx+1]  # [1, 2] normalized\n",
        "    \n",
        "    # Map extent (normalized coordinates)\n",
        "    map_extent = (0.0, 0.0, 1.0, 1.0)\n",
        "    \n",
        "    # Sample features at true position\n",
        "    sampled_features = differentiable_lookup(true_pos, radio_map, map_extent)\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"DIFFERENTIABLE BILINEAR RESAMPLING DEMONSTRATION\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nTrue Position (normalized): [{true_pos[0, 0].item():.4f}, {true_pos[0, 1].item():.4f}]\")\n",
        "    print(f\"Radio Map Shape: {radio_map.shape}\")\n",
        "    print(f\"\\nSampled Features at True Position:\")\n",
        "    \n",
        "    feature_names = ['Path Gain', 'ToA', 'AoA', 'SNR', 'SINR']\n",
        "    for i, (name, val) in enumerate(zip(feature_names, sampled_features[0].cpu().numpy())):\n",
        "        print(f\"  {name:12s}: {val:.4f}\")\n",
        "    \n",
        "    return sampled_features, radio_map\n",
        "\n",
        "sampled_features, radio_map_sample = demonstrate_bilinear_resampling(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a1d781",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_bilinear_sampling(batch, sample_idx=0):\n",
        "    \"\"\"Visualize the bilinear sampling process on the radio map.\"\"\"\n",
        "    radio_map = batch['radio_map'][sample_idx].cpu().numpy()  # [C, H, W]\n",
        "    true_pos = batch['position'][sample_idx].cpu().numpy()  # [2]\n",
        "    pred_pos = outputs['predicted_position'][sample_idx].cpu().numpy()\n",
        "    \n",
        "    h, w = radio_map.shape[-2:]\n",
        "    \n",
        "    # Sample grid of positions\n",
        "    grid_x = np.linspace(0, 1, 50)\n",
        "    grid_y = np.linspace(0, 1, 50)\n",
        "    xx, yy = np.meshgrid(grid_x, grid_y)\n",
        "    sample_positions = torch.tensor(np.stack([xx.flatten(), yy.flatten()], axis=1), dtype=torch.float32)\n",
        "    \n",
        "    # Expand radio map for batch\n",
        "    radio_map_tensor = batch['radio_map'][sample_idx:sample_idx+1].expand(len(sample_positions), -1, -1, -1)\n",
        "    \n",
        "    # Sample features at all positions\n",
        "    sampled = differentiable_lookup(sample_positions.to(batch['radio_map'].device), \n",
        "                                    radio_map_tensor, \n",
        "                                    (0.0, 0.0, 1.0, 1.0))\n",
        "    \n",
        "    # Reshape for visualization (using first feature - path gain)\n",
        "    sampled_map = sampled[:, 0].cpu().numpy().reshape(50, 50)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "    \n",
        "    # 1. Original radio map (Path Gain)\n",
        "    ax = axes[0]\n",
        "    im = ax.imshow(radio_map[0], cmap='inferno', origin='lower')\n",
        "    ax.scatter(true_pos[0] * w, true_pos[1] * h, c='lime', s=100, marker='*', \n",
        "               edgecolors='black', label='True Pos', zorder=10)\n",
        "    ax.scatter(pred_pos[0] * w, pred_pos[1] * h, c='red', s=80, marker='x', \n",
        "               linewidth=3, label='Pred Pos', zorder=10)\n",
        "    ax.set_title('Original Radio Map (Path Gain)', fontsize=12)\n",
        "    plt.colorbar(im, ax=ax)\n",
        "    ax.legend()\n",
        "    \n",
        "    # 2. Resampled via bilinear interpolation\n",
        "    ax = axes[1]\n",
        "    im = ax.imshow(sampled_map, cmap='inferno', origin='lower', extent=[0, 1, 0, 1])\n",
        "    ax.scatter(true_pos[0], true_pos[1], c='lime', s=100, marker='*', \n",
        "               edgecolors='black', label='True Pos', zorder=10)\n",
        "    ax.scatter(pred_pos[0], pred_pos[1], c='red', s=80, marker='x', \n",
        "               linewidth=3, label='Pred Pos', zorder=10)\n",
        "    ax.set_title('Bilinear Resampled (50x50 grid)', fontsize=12)\n",
        "    plt.colorbar(im, ax=ax)\n",
        "    ax.legend()\n",
        "    \n",
        "    # 3. Gradient visualization (gradient of path gain w.r.t. position)\n",
        "    ax = axes[2]\n",
        "    \n",
        "    # Compute gradient numerically\n",
        "    eps = 0.01\n",
        "    dx = np.zeros_like(sampled_map)\n",
        "    dy = np.zeros_like(sampled_map)\n",
        "    \n",
        "    for i in range(1, sampled_map.shape[0]-1):\n",
        "        for j in range(1, sampled_map.shape[1]-1):\n",
        "            dx[i, j] = (sampled_map[i, j+1] - sampled_map[i, j-1]) / (2 * (grid_x[1] - grid_x[0]))\n",
        "            dy[i, j] = (sampled_map[i+1, j] - sampled_map[i-1, j]) / (2 * (grid_y[1] - grid_y[0]))\n",
        "    \n",
        "    grad_mag = np.sqrt(dx**2 + dy**2)\n",
        "    im = ax.imshow(grad_mag, cmap='viridis', origin='lower', extent=[0, 1, 0, 1])\n",
        "    \n",
        "    # Overlay gradient arrows (subsampled)\n",
        "    step = 5\n",
        "    ax.quiver(xx[::step, ::step], yy[::step, ::step], \n",
        "              dx[::step, ::step], dy[::step, ::step], \n",
        "              color='white', alpha=0.7, scale=50)\n",
        "    \n",
        "    ax.scatter(true_pos[0], true_pos[1], c='lime', s=100, marker='*', \n",
        "               edgecolors='black', zorder=10)\n",
        "    ax.set_title('Gradient Magnitude (for physics loss)', fontsize=12)\n",
        "    plt.colorbar(im, ax=ax, label='|\u2207f|')\n",
        "    \n",
        "    plt.suptitle('Differentiable Bilinear Resampling', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_bilinear_sampling(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7e7cb9e",
      "metadata": {},
      "source": [
        "### \ud83d\udcca Interpretation: Bilinear Interpolation Visualization\n",
        "\n",
        "**What to look for:**\n",
        "- **Smooth Gradients**: Resampled map should show smooth interpolation between grid points\n",
        "- **Gradient Direction**: Arrows should point toward areas of higher signal strength\n",
        "- **Gradient Magnitude**: Should be strongest at boundaries (e.g., building edges, signal transitions)\n",
        "- **Position Correspondence**: True and predicted positions should align with gradient patterns\n",
        "\n",
        "**Physics Loss Validation:**\n",
        "- **Strong gradients near positions**: Indicates the loss can effectively guide the model\n",
        "- **Smooth interpolation**: Ensures differentiability for backpropagation\n",
        "- **Consistent sampling**: Sampled features should match the local radio map values\n",
        "\n",
        "**Common Issues:**\n",
        "- **Discontinuous gradients**: May indicate numerical instability in bilinear interpolation\n",
        "- **Weak gradients everywhere**: Radio map may be too uniform - check data generation\n",
        "- **Misaligned positions**: Coordinate transformation issue between map and position space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4fb579d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_physics_loss_demo(batch, outputs, sample_idx=0):\n",
        "    \"\"\"Demonstrate physics loss computation.\"\"\"\n",
        "    \n",
        "    # Get the number of radio map channels\n",
        "    num_channels = batch['radio_map'].shape[1]\n",
        "    \n",
        "    # Feature names matching the radio map channels\n",
        "    channel_names = ('path_gain', 'toa', 'aoa', 'snr', 'sinr')[:num_channels]\n",
        "    \n",
        "    # Setup physics loss with feature weights from the project\n",
        "    config = PhysicsLossConfig(\n",
        "        feature_weights={\n",
        "            'path_gain': 1.0,\n",
        "            'toa': 0.5,\n",
        "            'aoa': 0.3,\n",
        "            'snr': 0.8,\n",
        "            'sinr': 0.8,\n",
        "        },\n",
        "        map_extent=(0.0, 0.0, 1.0, 1.0),\n",
        "        loss_type='mse',\n",
        "        normalize_features=False,\n",
        "        channel_names=channel_names,\n",
        "    )\n",
        "    physics_loss_fn = PhysicsLoss(config).to(batch['radio_map'].device)\n",
        "    \n",
        "    # Get predicted positions\n",
        "    pred_pos = outputs['predicted_position']  # [B, 2]\n",
        "    true_pos = batch['position']  # [B, 2]\n",
        "    \n",
        "    # Extract observed features from measurements (simplified)\n",
        "    # In practice, these come from the actual measurements\n",
        "    observed_features = differentiable_lookup(\n",
        "        true_pos, \n",
        "        batch['radio_map'], \n",
        "        (0.0, 0.0, 1.0, 1.0)\n",
        "    )\n",
        "    \n",
        "    # Compute physics loss for predicted positions\n",
        "    physics_loss_pred = physics_loss_fn(pred_pos, observed_features, batch['radio_map'])\n",
        "    \n",
        "    # Compute physics loss for ground truth (should be ~0)\n",
        "    physics_loss_gt = physics_loss_fn(true_pos, observed_features, batch['radio_map'])\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"PHYSICS LOSS COMPUTATION\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nPhysics Loss at Predicted Positions: {physics_loss_pred.item():.6f}\")\n",
        "    print(f\"Physics Loss at Ground Truth:        {physics_loss_gt.item():.6f}\")\n",
        "    print(f\"Difference:                          {(physics_loss_pred - physics_loss_gt).item():.6f}\")\n",
        "    print(\"\\nNote: Ground truth loss should be near 0 (features match exactly)\")\n",
        "    print(\"      The loss difference indicates how much the prediction deviates\")\n",
        "    print(\"      from physics-consistent positions.\")\n",
        "    \n",
        "    # Visualize feature comparison\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FEATURE COMPARISON (Sample 0)\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Sample features at predicted and true positions\n",
        "    pred_features = differentiable_lookup(pred_pos, batch['radio_map'], (0.0, 0.0, 1.0, 1.0))\n",
        "    true_features = observed_features\n",
        "    \n",
        "    print(f\"\\n{'Feature':15s} | {'Observed':>12s} | {'At Predicted':>12s} | {'Residual':>12s}\")\n",
        "    print(\"-\" * 60)\n",
        "    for i, name in enumerate(channel_names):\n",
        "        obs = true_features[0, i].item()\n",
        "        pred = pred_features[0, i].item()\n",
        "        residual = abs(obs - pred)\n",
        "        print(f\"{name:15s} | {obs:12.4f} | {pred:12.4f} | {residual:12.4f}\")\n",
        "    \n",
        "    return physics_loss_pred, physics_loss_gt\n",
        "\n",
        "physics_loss_pred, physics_loss_gt = compute_physics_loss_demo(batch, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3810ee7",
      "metadata": {},
      "source": [
        "## 9. Inference-Time Position Refinement (MAP Refinement)\n",
        "\n",
        "**Plots: Refinement Trajectory (2 panels)**\n",
        "\n",
        "At inference time, predictions can be refined via gradient descent:\n",
        "\n",
        "$$E(\\mathbf{y}) = \\mathcal{L}_{\\text{phys}}(\\mathbf{y}) + \\lambda_{\\text{dens}} \\cdot \\text{NLL}(\\mathbf{y} | \\text{network})$$\n",
        "\n",
        "1. **Trajectory on Map** - Shows initial (red) \u2192 refined (blue) path with ground truth (green star)\n",
        "2. **Error Comparison** - Bar chart showing error reduction with improvement annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "249cb70e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.physics_loss import refine_position, RefineConfig\n",
        "\n",
        "def demonstrate_position_refinement(batch, outputs, sample_idx=0, scene_extent=512.0):\n",
        "    \"\"\"Demonstrate inference-time position refinement using physics loss.\"\"\"\n",
        "    \n",
        "    # Get initial prediction\n",
        "    initial_pos = outputs['predicted_position'].clone()\n",
        "    true_pos = batch['position']\n",
        "    \n",
        "    # Extract observed features (from ground truth for demonstration)\n",
        "    observed_features = differentiable_lookup(\n",
        "        true_pos, \n",
        "        batch['radio_map'], \n",
        "        (0.0, 0.0, 1.0, 1.0)\n",
        "    )\n",
        "    \n",
        "    # Configure refinement\n",
        "    refine_config = RefineConfig(\n",
        "        num_steps=50,\n",
        "        learning_rate=0.01,\n",
        "        min_confidence_threshold=None,  # Refine all samples\n",
        "        density_weight=0.1,\n",
        "        clip_to_extent=True,\n",
        "        map_extent=(0.0, 0.0, 1.0, 1.0),\n",
        "        physics_config=PhysicsLossConfig(\n",
        "            map_extent=(0.0, 0.0, 1.0, 1.0),\n",
        "            loss_type='mse',\n",
        "        ),\n",
        "    )\n",
        "    \n",
        "    # Run refinement\n",
        "    refined_pos, refine_info = refine_position(\n",
        "        initial_xy=initial_pos,\n",
        "        observed_features=observed_features,\n",
        "        radio_maps=batch['radio_map'],\n",
        "        config=refine_config,\n",
        "    )\n",
        "    \n",
        "    # Compute errors\n",
        "    initial_errors = torch.norm((initial_pos - true_pos) * scene_extent, dim=1).cpu().numpy()\n",
        "    refined_errors = torch.norm((refined_pos - true_pos) * scene_extent, dim=1).cpu().numpy()\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"POSITION REFINEMENT RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nInitial Physics Loss:  {refine_info['loss_initial']:.6f}\")\n",
        "    print(f\"Final Physics Loss:    {refine_info['loss_final']:.6f}\")\n",
        "    print(f\"Samples Refined:       {refine_info['num_refined']}\")\n",
        "    print(f\"\\nError Comparison (meters):\")\n",
        "    print(f\"  Initial Mean Error:  {initial_errors.mean():.2f} m\")\n",
        "    print(f\"  Refined Mean Error:  {refined_errors.mean():.2f} m\")\n",
        "    print(f\"  Improvement:         {initial_errors.mean() - refined_errors.mean():.2f} m\")\n",
        "    \n",
        "    return initial_pos, refined_pos, initial_errors, refined_errors\n",
        "\n",
        "initial_pos, refined_pos, initial_errors, refined_errors = demonstrate_position_refinement(batch, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e32415f5",
      "metadata": {},
      "source": [
        "### \ud83d\udcca Interpretation: Physics Loss Computation\n",
        "\n",
        "**What to look for:**\n",
        "- **Loss at GT \u2248 0**: Physics loss should be near zero at ground truth position\n",
        "- **Loss at Prediction > Loss at GT**: Predicted position should have higher loss than ground truth\n",
        "- **Per-channel contributions**: Dominant channels (path_gain, SINR) should contribute most to the loss\n",
        "- **Feature residuals**: Differences should be small (<0.1) for good predictions\n",
        "\n",
        "**Physics Loss Validation:**\n",
        "- **GT Loss < 0.01**: Excellent consistency between observations and radio map\n",
        "- **GT Loss > 0.1**: Potential mismatch between measurement simulation and radio map generation\n",
        "- **Prediction Loss < 0.5**: Model prediction is physically plausible\n",
        "- **Prediction Loss > 2.0**: Model prediction contradicts physics - may be overfitting to data-driven patterns\n",
        "\n",
        "**Common Issues:**\n",
        "- **High GT loss**: Indicates inconsistency in data generation - radio map and measurements don't align\n",
        "- **Similar loss at prediction and GT**: Physics loss may not be discriminative enough (check feature weights)\n",
        "- **Negative contributions**: Feature weight tuning needed or normalization issue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "810da386",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_refinement(batch, initial_pos, refined_pos, sample_idx=0, scene_extent=512.0):\n",
        "    \"\"\"Visualize the position refinement trajectory.\"\"\"\n",
        "    true_pos = batch['position'][sample_idx].cpu().numpy()\n",
        "    init_pos = initial_pos[sample_idx].cpu().numpy()\n",
        "    ref_pos = refined_pos[sample_idx].cpu().numpy()\n",
        "    \n",
        "    radio_map = batch['radio_map'][sample_idx, 0].cpu().numpy()  # Path Gain\n",
        "    h, w = radio_map.shape\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # 1. Visualization on radio map\n",
        "    ax = axes[0]\n",
        "    im = ax.imshow(radio_map, cmap='inferno', origin='lower', extent=[0, 1, 0, 1])\n",
        "    \n",
        "    # Draw arrow from initial to refined\n",
        "    ax.annotate('', xy=(ref_pos[0], ref_pos[1]), xytext=(init_pos[0], init_pos[1]),\n",
        "                arrowprops=dict(arrowstyle='->', color='cyan', lw=2))\n",
        "    \n",
        "    ax.scatter(true_pos[0], true_pos[1], c='lime', s=150, marker='*', \n",
        "               edgecolors='black', label='Ground Truth', zorder=10)\n",
        "    ax.scatter(init_pos[0], init_pos[1], c='red', s=80, marker='o', \n",
        "               edgecolors='black', label='Initial Pred', zorder=9)\n",
        "    ax.scatter(ref_pos[0], ref_pos[1], c='blue', s=80, marker='s', \n",
        "               edgecolors='black', label='Refined Pred', zorder=9)\n",
        "    \n",
        "    ax.set_xlim([0, 1])\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.set_title(f'Position Refinement (Sample {sample_idx})', fontsize=12)\n",
        "    ax.set_xlabel('X (normalized)')\n",
        "    ax.set_ylabel('Y (normalized)')\n",
        "    plt.colorbar(im, ax=ax, label='Path Gain')\n",
        "    ax.legend(loc='upper right')\n",
        "    \n",
        "    # 2. Error comparison bar chart\n",
        "    ax = axes[1]\n",
        "    \n",
        "    init_error = np.linalg.norm(init_pos - true_pos) * scene_extent\n",
        "    ref_error = np.linalg.norm(ref_pos - true_pos) * scene_extent\n",
        "    \n",
        "    bars = ax.bar(['Initial', 'Refined'], [init_error, ref_error], \n",
        "                  color=['coral', 'steelblue'], edgecolor='black')\n",
        "    \n",
        "    # Add improvement annotation\n",
        "    improvement = init_error - ref_error\n",
        "    improvement_pct = (improvement / init_error) * 100 if init_error > 0 else 0\n",
        "    \n",
        "    ax.annotate(f'\u0394 = {improvement:.1f}m ({improvement_pct:.1f}%)',\n",
        "                xy=(1, ref_error), xytext=(1.3, (init_error + ref_error) / 2),\n",
        "                fontsize=11, ha='left',\n",
        "                arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\n",
        "    \n",
        "    ax.set_ylabel('Localization Error (m)', fontsize=12)\n",
        "    ax.set_title('Error Before/After Refinement', fontsize=12)\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Annotate bars\n",
        "    for bar, val in zip(bars, [init_error, ref_error]):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
        "                f'{val:.1f}m', ha='center', va='bottom', fontsize=11)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_refinement(batch, initial_pos, refined_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46475a84",
      "metadata": {},
      "source": [
        "### \ud83d\udcca Interpretation: Physics-Guided Position Refinement\n",
        "\n",
        "**What to look for:**\n",
        "- **Trajectory Convergence**: Blue path should move closer to green star (ground truth)\n",
        "- **Error Reduction**: Bar chart should show improvement (blue bar shorter than red)\n",
        "- **Smooth Trajectory**: Refinement path should be continuous without erratic jumps\n",
        "- **Final Position**: Refined position should be closer to GT than initial prediction\n",
        "\n",
        "**Refinement Performance:**\n",
        "- **Error reduction > 20%**: Excellent - physics loss successfully guides refinement\n",
        "- **Error reduction 5-20%**: Good - modest improvement from physics constraints\n",
        "- **Error reduction < 5%**: Minimal - either initial prediction was already good or physics loss isn't helping\n",
        "- **Error increase**: Physics loss may be misaligned with true environment (check data consistency)\n",
        "\n",
        "**Common Issues:**\n",
        "- **Refinement diverges**: Learning rate too high or physics loss has local minima\n",
        "- **No improvement**: Feature weights may need tuning, or radio map quality is poor\n",
        "- **Overshooting GT**: Density weight too low - model ignores network's probabilistic prior\n",
        "- **Stuck at initial position**: Learning rate too low or gradient vanishing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "853bbc42",
      "metadata": {},
      "source": [
        "## 10. Position Refinement with Physics Loss\n",
        "\n",
        "**Demonstration: Inference-Time Optimization**\n",
        "\n",
        "After initial prediction, we can refine positions using gradient descent on the physics loss:\n",
        "\n",
        "$$\\hat{\\mathbf{x}}_{t+1} = \\hat{\\mathbf{x}}_t - \\eta \\nabla_{\\mathbf{x}} \\mathcal{L}_{\\text{phys}}(\\hat{\\mathbf{x}}_t)$$\n",
        "\n",
        "This optimization uses differentiable bilinear interpolation to:\n",
        "1. Sample radio map features at predicted position\n",
        "2. Compare with observed measurements\n",
        "3. Update position to minimize discrepancy\n",
        "\n",
        "Visualizations show the refinement trajectory and error improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ae4e4f1",
      "metadata": {},
      "source": [
        "## 11. Summary Dashboard\n",
        "\n",
        "**Plot: Multi-Panel Summary (5 components)**\n",
        "\n",
        "Comprehensive overview combining all key results:\n",
        "\n",
        "1. **Metrics Table** - Median, RMSE, 67th/90th percentiles\n",
        "2. **Loss Pie Chart** - Coarse vs Fine loss contribution breakdown\n",
        "3. **Per-Sample Bars** - Initial vs Refined error for each sample\n",
        "4. **CDF Comparison** - Before/After refinement curves\n",
        "5. **Architecture Diagram** - Text summary of model components\n",
        "\n",
        "### Total Training Loss\n",
        "\n",
        "$$\\mathcal{L}_{\\text{total}} = \\underbrace{\\lambda_{\\text{coarse}} \\mathcal{L}_{\\text{coarse}}}_{\\text{Cross-Entropy}} + \\underbrace{\\lambda_{\\text{fine}} \\mathcal{L}_{\\text{fine}}}_{\\text{Mixture NLL}} + \\underbrace{\\lambda_{\\text{phys}} \\mathcal{L}_{\\text{phys}}}_{\\text{Physics Consistency}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee1d08d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_summary_visualization(metrics, losses, initial_errors, refined_errors, scene_extent=512.0):\n",
        "    \"\"\"Create a comprehensive summary visualization.\"\"\"\n",
        "    \n",
        "    fig = plt.figure(figsize=(16, 10))\n",
        "    \n",
        "    # Create grid layout\n",
        "    gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    # 1. Metrics table\n",
        "    ax = fig.add_subplot(gs[0, 0])\n",
        "    ax.axis('off')\n",
        "    \n",
        "    table_data = [\n",
        "        ['Metric', 'Value'],\n",
        "        ['Median Error', f\"{metrics['Median Error (m)']:.2f} m\"],\n",
        "        ['RMSE', f\"{metrics['RMSE (m)']:.2f} m\"],\n",
        "        ['67th Percentile', f\"{metrics['67th Percentile (m)']:.2f} m\"],\n",
        "        ['90th Percentile', f\"{metrics['90th Percentile (m)']:.2f} m\"],\n",
        "    ]\n",
        "    \n",
        "    table = ax.table(cellText=table_data, loc='center', cellLoc='center',\n",
        "                     colWidths=[0.5, 0.5])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(11)\n",
        "    table.scale(1.2, 1.5)\n",
        "    \n",
        "    # Color header row\n",
        "    for i in range(2):\n",
        "        table[(0, i)].set_facecolor('#4472C4')\n",
        "        table[(0, i)].set_text_props(color='white', weight='bold')\n",
        "    \n",
        "    ax.set_title('Evaluation Metrics', fontsize=14, pad=20)\n",
        "    \n",
        "    # 2. Loss breakdown pie chart\n",
        "    ax = fig.add_subplot(gs[0, 1])\n",
        "    loss_values = [losses['coarse_loss'].item(), losses['fine_loss'].item()]\n",
        "    loss_labels = ['Coarse\\n(CE)', 'Fine\\n(NLL)']\n",
        "    colors = ['#FF6B6B', '#4ECDC4']\n",
        "    \n",
        "    wedges, texts, autotexts = ax.pie(loss_values, labels=loss_labels, colors=colors,\n",
        "                                       autopct='%1.1f%%', startangle=90,\n",
        "                                       explode=(0.05, 0.05))\n",
        "    ax.set_title('Loss Breakdown', fontsize=14)\n",
        "    \n",
        "    # 3. Before/After Refinement comparison\n",
        "    ax = fig.add_subplot(gs[0, 2])\n",
        "    \n",
        "    x = np.arange(len(initial_errors))\n",
        "    width = 0.35\n",
        "    \n",
        "    ax.bar(x - width/2, initial_errors, width, label='Initial', color='coral', alpha=0.8)\n",
        "    ax.bar(x + width/2, refined_errors, width, label='Refined', color='steelblue', alpha=0.8)\n",
        "    \n",
        "    ax.set_xlabel('Sample Index')\n",
        "    ax.set_ylabel('Error (m)')\n",
        "    ax.set_title('Per-Sample Refinement Improvement', fontsize=14)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # 4. CDF comparison\n",
        "    ax = fig.add_subplot(gs[1, :2])\n",
        "    \n",
        "    sorted_init = np.sort(initial_errors)\n",
        "    sorted_ref = np.sort(refined_errors)\n",
        "    cdf = np.arange(1, len(sorted_init) + 1) / len(sorted_init) * 100\n",
        "    \n",
        "    ax.plot(sorted_init, cdf, 'o-', linewidth=2, label='Initial', color='coral', markersize=8)\n",
        "    ax.plot(sorted_ref, cdf, 's-', linewidth=2, label='After Refinement', color='steelblue', markersize=8)\n",
        "    \n",
        "    ax.axhline(67, color='gray', linestyle='--', alpha=0.5)\n",
        "    ax.axhline(90, color='gray', linestyle='--', alpha=0.5)\n",
        "    ax.text(ax.get_xlim()[1] * 0.98, 67, '67%', ha='right', va='bottom', fontsize=10)\n",
        "    ax.text(ax.get_xlim()[1] * 0.98, 90, '90%', ha='right', va='bottom', fontsize=10)\n",
        "    \n",
        "    ax.set_xlabel('Localization Error (m)', fontsize=12)\n",
        "    ax.set_ylabel('CDF (%)', fontsize=12)\n",
        "    ax.set_title('Error CDF: Before vs After Physics Refinement', fontsize=14)\n",
        "    ax.legend(fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_ylim([0, 105])\n",
        "    \n",
        "    # 5. Model architecture summary\n",
        "    ax = fig.add_subplot(gs[1, 2])\n",
        "    ax.axis('off')\n",
        "    \n",
        "    arch_text = \"\"\"\n",
        "    Model Architecture\n",
        "    \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
        "    \n",
        "    1. Radio Encoder\n",
        "       \u2514\u2500 Set Transformer\n",
        "    \n",
        "    2. Map Encoder\n",
        "       \u2514\u2500 ViT (E2-Equivariant)\n",
        "    \n",
        "    3. Cross-Attention Fusion\n",
        "       \u2514\u2500 Multi-head attention\n",
        "    \n",
        "    4. Coarse Head\n",
        "       \u2514\u2500 Grid cell classifier\n",
        "    \n",
        "    5. Fine Head\n",
        "       \u2514\u2500 GMM (\u03bc, \u03c3) predictor\n",
        "    \n",
        "    6. Physics Regularization\n",
        "       \u2514\u2500 Differentiable lookup\n",
        "    \"\"\"\n",
        "    \n",
        "    ax.text(0.1, 0.95, arch_text, transform=ax.transAxes, fontsize=10,\n",
        "            verticalalignment='top', fontfamily='monospace',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    plt.suptitle('UE Localization Pipeline - Summary', fontsize=16, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create final summary\n",
        "create_summary_visualization(metrics, losses, initial_errors, refined_errors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77dbb76b",
      "metadata": {},
      "source": [
        "### \ud83d\udcca Interpretation: Pipeline Summary Dashboard\n",
        "\n",
        "**What to look for:**\n",
        "- **Consistent Performance**: All metrics and visualizations should tell a coherent story\n",
        "- **Error Distribution**: Should match the overall performance expectations (median vs mean)\n",
        "- **Refinement Impact**: Compare initial vs refined errors to assess physics loss effectiveness\n",
        "- **Coarse Accuracy**: High top-1/top-3 accuracy indicates the coarse stage is working well\n",
        "\n",
        "**Overall Assessment Checklist:**\n",
        "1. **Data Quality**: Check that maps show realistic patterns and proper alignment\n",
        "2. **Model Convergence**: Losses should be reasonable (coarse < 3, fine < 5)\n",
        "3. **Prediction Accuracy**: Median error should align with performance targets\n",
        "4. **Physics Consistency**: Physics loss at GT should be near zero\n",
        "5. **Refinement Benefit**: Refinement should improve or maintain accuracy\n",
        "\n",
        "**Next Steps Based on Results:**\n",
        "- **Poor accuracy (>50m)**: Retrain model, check data quality, or add more training scenes\n",
        "- **High coarse loss**: Increase coarse head capacity or adjust loss weights\n",
        "- **Diffuse predictions**: Add more discriminative features or use attention visualization\n",
        "- **Physics loss mismatch**: Tune feature weights or validate data generation consistency"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}