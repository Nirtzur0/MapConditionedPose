"""
LMDB Dataset Writer for Multi-Layer Features
Efficient key-value storage with perfect multiprocessing support
"""

import numpy as np
import lmdb
import pickle
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
import logging
import json

logger = logging.getLogger(__name__)


class LMDBDatasetWriter:
    """
    Writes multi-layer features to LMDB dataset.
    
    Each sample is stored as a pickled dictionary with all data.
    Metadata is stored separately for efficient querying.
    
    Sample structure:
        {
            'rt_features': dict with path_gains, path_delays, etc.
            'phy_features': dict with rsrp, rsrq, sinr, etc.
            'mac_features': dict with cell_ids, timing_advance, etc.
            'position': (x, y, z)
            'timestamp': float
            'scene_id': str
            'ue_id': int
            'radio_map': array [C, H, W] or scene_idx
            'osm_map': array [C, H, W] or scene_idx
        }
    
    Metadata structure:
        {
            'num_samples': int
            'scene_ids': list of unique scene IDs
            'scene_maps': dict mapping scene_id -> {'radio_map': array, 'osm_map': array}
            'normalization_stats': dict with mean/std for each feature
            'split_indices': dict with train/val/test indices (if applicable)
            'max_dimensions': dict with max_cells, max_beams, max_paths
        }
    """
    
    def __init__(self, 
                 output_dir: Path,
                 map_size: int = 100 * 1024**3,  # 100GB default
                 split_name: Optional[str] = None):
        """
        Args:
            output_dir: Output directory for LMDB database
            map_size: Maximum database size in bytes (default 100GB)
            split_name: Name of split (train/val/test), None for single dataset
        """
        self.output_dir = Path(output_dir)
        self.map_size = map_size
        self.split_name = split_name
        
        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize LMDB
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.db_name = f"dataset_{timestamp}"
        if split_name:
            self.db_name += f"_{split_name}"
        
        self.db_path = self.output_dir / f"{self.db_name}.lmdb"
        
        logger.info(f"Creating LMDB database at {self.db_path}")
        self.env = lmdb.open(
            str(self.db_path),
            map_size=map_size,
            max_dbs=2,  # Main DB + metadata DB
            meminit=False,
            map_async=True,
        )
        
        # Sample counter
        self.sample_count = 0
        
        # Track dimensions
        self.max_dimensions = {}
        
        # Track scene maps (store once per scene)
        self.scene_maps = {}
        self.scene_id_to_idx = {}
        
        # Accumulate data for normalization stats
        self.feature_accumulators = {
            'rt': [],
            'phy': [],
            'mac': []
        }
        
        logger.info(f"LMDB Writer initialized: {self.db_path}")
    
    def set_max_dimensions(self, dimensions: Dict[str, int]):
        """Set maximum dimensions for arrays (max_cells, max_beams, max_paths)."""
        self.max_dimensions.update(dimensions)
        logger.info(f"Max dimensions set: {self.max_dimensions}")
    
    def write_scene_maps(self, scene_id: str, radio_map: np.ndarray, osm_map: np.ndarray):
        """Store maps for a scene (called once per scene)."""
        if scene_id not in self.scene_maps:
            scene_idx = len(self.scene_maps)
            self.scene_maps[scene_id] = {
                'radio_map': radio_map.astype(np.float32),
                'osm_map': osm_map.astype(np.float32),
                'scene_idx': scene_idx
            }
            self.scene_id_to_idx[scene_id] = scene_idx
            logger.debug(f"Stored maps for scene {scene_id} (index {scene_idx})")
    
    def append(self, sample_data: Dict[str, Any], scene_id: str, scene_metadata: Optional[Dict] = None):
        """
        Append a sample to the database.
        
        Args:
            sample_data: Dictionary with rt_layer, phy_fapi_layer, mac_rrc_layer, positions, timestamps
            scene_id: Scene identifier
            scene_metadata: Optional metadata about the scene
        """
        # Build sample dictionary
        sample = {
            'scene_id': scene_id,
            'scene_idx': self.scene_id_to_idx.get(scene_id, -1),
        }
        
        # Extract RT features
        if 'rt_layer' in sample_data:
            rt = sample_data['rt_layer']
            sample['rt_features'] = {
                'path_gains': rt.get('path_gains', np.array([], dtype=np.complex64)),
                'path_delays': rt.get('path_delays', np.array([], dtype=np.float32)),
                'path_aoa_azimuth': rt.get('path_aoa_azimuth', np.array([], dtype=np.float32)),
                'path_aoa_elevation': rt.get('path_aoa_elevation', np.array([], dtype=np.float32)),
                'path_aod_azimuth': rt.get('path_aod_azimuth', np.array([], dtype=np.float32)),
                'path_aod_elevation': rt.get('path_aod_elevation', np.array([], dtype=np.float32)),
                'path_doppler': rt.get('path_doppler', np.array([], dtype=np.float32)),
                'rms_delay_spread': rt.get('rms_delay_spread', 0.0),
                'k_factor': rt.get('k_factor', 0.0),
                'num_paths': rt.get('num_paths', 0),
            }
            # Accumulate for normalization
            self._accumulate_features('rt', sample['rt_features'])
        
        # Extract PHY features
        if 'phy_fapi_layer' in sample_data:
            phy = sample_data['phy_fapi_layer']
            sample['phy_features'] = {
                'rsrp': phy.get('rsrp', np.array([], dtype=np.float32)),
                'rsrq': phy.get('rsrq', np.array([], dtype=np.float32)),
                'sinr': phy.get('sinr', np.array([], dtype=np.float32)),
                'cqi': phy.get('cqi', np.array([], dtype=np.int32)),
                'ri': phy.get('ri', np.array([], dtype=np.int32)),
                'pmi': phy.get('pmi', np.array([], dtype=np.int32)),
                'l1_rsrp_beams': phy.get('l1_rsrp_beams', np.array([], dtype=np.float32)),
                'best_beam_ids': phy.get('best_beam_ids', np.array([], dtype=np.int32)),
            }
            self._accumulate_features('phy', sample['phy_features'])
        
        # Extract MAC features
        if 'mac_rrc_layer' in sample_data:
            mac = sample_data['mac_rrc_layer']
            sample['mac_features'] = {
                'serving_cell_id': mac.get('serving_cell_id', -1),
                'neighbor_cell_ids': mac.get('neighbor_cell_ids', np.array([], dtype=np.int32)),
                'timing_advance': mac.get('timing_advance', 0),
                'phr': mac.get('phr', 0.0),
                'throughput': mac.get('dl_throughput_mbps', 0.0),
                'bler': mac.get('bler', 0.0),
            }
            self._accumulate_features('mac', sample['mac_features'])
        
        # Extract position
        if 'positions' in sample_data:
            pos = sample_data['positions']
            sample['position'] = (
                float(pos.get('ue_x', 0.0)),
                float(pos.get('ue_y', 0.0)),
                float(pos.get('ue_z', 0.0))
            )
        
        # Extract timestamp
        if 'timestamps' in sample_data:
            sample['timestamp'] = float(sample_data['timestamps'].get('t', 0.0))
        
        # Extract UE ID
        if 'metadata' in sample_data:
            sample['ue_id'] = int(sample_data['metadata'].get('ue_id', 0))
        
        # Add scene metadata if provided
        if scene_metadata:
            sample['scene_metadata'] = scene_metadata
        
        # Write to LMDB
        with self.env.begin(write=True) as txn:
            key = f'sample_{self.sample_count:08d}'.encode()
            value = pickle.dumps(sample, protocol=pickle.HIGHEST_PROTOCOL)
            txn.put(key, value)
        
        self.sample_count += 1
        
        if self.sample_count % 1000 == 0:
            logger.info(f"Written {self.sample_count} samples")
    
    def _accumulate_features(self, layer: str, features: Dict):
        """Accumulate features for computing normalization statistics."""
        # Sample every 10th sample to save memory
        if self.sample_count % 10 == 0:
            self.feature_accumulators[layer].append(features.copy())
    
    def finalize(self) -> Path:
        """
        Finalize the dataset: compute stats, write metadata.
        
        Returns:
            Path to the LMDB database
        """
        logger.info(f"Finalizing dataset with {self.sample_count} samples")
        
        # Compute normalization statistics
        norm_stats = self._compute_normalization_stats()
        
        # Prepare metadata
        metadata = {
            'num_samples': self.sample_count,
            'scene_ids': list(self.scene_maps.keys()),
            'scene_id_to_idx': self.scene_id_to_idx,
            'max_dimensions': self.max_dimensions,
            'normalization_stats': norm_stats,
            'created_at': datetime.now().isoformat(),
            'split_name': self.split_name,
        }
        
        # Write metadata to LMDB
        with self.env.begin(write=True) as txn:
            # Store metadata
            txn.put(b'__metadata__', pickle.dumps(metadata, protocol=pickle.HIGHEST_PROTOCOL))
            
            # Store scene maps
            for scene_id, scene_data in self.scene_maps.items():
                key = f'__scene_map_{scene_id}__'.encode()
                txn.put(key, pickle.dumps(scene_data, protocol=pickle.HIGHEST_PROTOCOL))
        
        # Sync to disk
        self.env.sync()
        self.env.close()
        
        logger.info(f"Dataset finalized: {self.db_path}")
        logger.info(f"Total samples: {self.sample_count}")
        logger.info(f"Total scenes: {len(self.scene_maps)}")
        logger.info(f"Database size: {self.db_path.stat().st_size / 1024**2:.1f} MB")
        
        return self.db_path
    
    def _compute_normalization_stats(self) -> Dict:
        """Compute mean and std for normalization."""
        stats = {}
        
        for layer, samples in self.feature_accumulators.items():
            if not samples:
                continue
            
            layer_stats = {}
            
            # Get all keys from first sample
            sample_keys = samples[0].keys()
            
            for key in sample_keys:
                # Collect all values for this key
                values = []
                for sample in samples:
                    val = sample[key]
                    if isinstance(val, np.ndarray) and val.size > 0:
                        values.extend(val.flatten())
                    elif isinstance(val, (int, float)) and val != 0:
                        values.append(val)
                
                if values:
                    values = np.array(values)
                    layer_stats[key] = {
                        'mean': float(np.mean(values)),
                        'std': float(np.std(values)),
                        'min': float(np.min(values)),
                        'max': float(np.max(values)),
                    }
            
            stats[layer] = layer_stats
        
        logger.info(f"Computed normalization stats for {len(stats)} layers")
        return stats
